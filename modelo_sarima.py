# -*- coding: utf-8 -*-
"""Modelo_Sarima.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iAFMXztN6AvkjVaC3_b2wtbi-jGTxS8R
"""

import os
import pickle
import json
import warnings
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from datetime import datetime, timedelta

warnings.filterwarnings("ignore")

# ======================================================
# URL DE GOOGLE SHEETS (definida a nivel global)
# ======================================================
SHEET_URL = "https://docs.google.com/spreadsheets/d/1x1FeUolFWlR07tgrc6F4cgeUhJYV7uQ5yuRTBHO8jWI/edit?gid=0#gid=0"

# ======================================================
# CONFIGURACI√ìN DE CARPETA DE SALIDA SIMPLIFICADA
# ======================================================
def configurar_carpeta_salida():
    """Configura la carpeta donde se guardar√°n los resultados JSON"""
    try:
        # En Colab, guardar en Drive
        if 'IN_COLAB' in globals() and IN_COLAB:
            carpeta_base = "/content/drive/MyDrive/pronosticos"
        else:
            # En GitHub Actions o local
            carpeta_base = "./pronosticos"
        
        # Crear solo una carpeta
        os.makedirs(carpeta_base, exist_ok=True)
        print(f"‚úÖ Carpeta creada/verificada: {carpeta_base}")
        
        return carpeta_base
    except Exception as e:
        print(f"‚ö†Ô∏è  Error configurando carpeta: {e}")
        return None

# ======================================================
# 1. FUNCI√ìN DE CONEXI√ìN A GOOGLE SHEETS (COLAB + GITHUB)
# ======================================================


def conectar_a_google_sheets():
    """
    Conecta a Google Sheets de manera inteligente
    - En Colab: usa autenticaci√≥n normal
    - En GitHub: puede usar Service Account
    """

    try:
        # Verificar si estamos en Google Colab
        try:
            from google.colab import auth

            IN_COLAB = True
        except:
            IN_COLAB = False

        if IN_COLAB:
            # ========== MODO COLAB ==========
            print("üîë Autenticando en Google Colab...")

            # Autenticaci√≥n interactiva
            auth.authenticate_user()
            from google.auth import default

            creds, _ = default()

            import gspread

            gc = gspread.authorize(creds)

        else:
            # ========== MODO GITHUB/LOCAL ==========
            import gspread

            # Intentar con variable de entorno (para GitHub Actions)
            creds_json = os.getenv("GOOGLE_SHEETS_CREDS")

            if creds_json:
                print("üîë Usando Service Account desde variable de entorno...")
                from google.oauth2.service_account import Credentials

                creds_dict = json.loads(creds_json)
                scope = ["https://www.googleapis.com/auth/spreadsheets"]
                credentials = Credentials.from_service_account_info(
                    creds_dict, scopes=scope
                )
                gc = gspread.authorize(credentials)
            else:
                # Intentar autenticaci√≥n normal (fallback)
                print("‚ö†Ô∏è  Intentando autenticaci√≥n normal...")
                gc = gspread.oauth()  # Esto abrir√° navegador en local

        # Abrir la hoja
        spreadsheet = gc.open_by_url(SHEET_URL)
        worksheet = spreadsheet.get_worksheet(0)

        print("‚úÖ Conexi√≥n exitosa a Google Sheets")
        return worksheet

    except Exception as e:
        print(f"‚ùå Error conectando a Google Sheets: {e}")
        print("‚ö†Ô∏è  Usando datos de ejemplo para continuar...")
        return None


# ======================================================
# 2. CONFIGURACI√ìN INICIAL DE COLAB
# ======================================================

# Montar Google Drive (solo funciona en Colab)
try:
    from google.colab import drive

    drive.mount("/content/drive", force_remount=False)
    IN_COLAB = True
    print("‚úÖ Google Drive montado")

except:
    IN_COLAB = False
    print("‚ö†Ô∏è  No es Google Colab, omitiendo montaje de Drive")

# ======================================================
# 3. INSTALAR DEPENDENCIAS (SOLO COLAB)
# ======================================================

if IN_COLAB:
    print("üì¶ Instalando dependencias en Colab...")
else:
    print("‚úÖ En GitHub, las dependencias se instalan desde requirements.txt")

# ======================================================
# 4. IMPORTAR BIBLIOTECAS RESTANTES
# ======================================================

import gspread
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
from statsmodels.tsa.statespace.sarimax import SARIMAX
import matplotlib.dates as mdates

# Cargar jupyter-black (solo Colab)
if IN_COLAB:
    import jupyter_black

    jupyter_black.load()

# ======================================================
# 5. REEMPLAZAR LA IMPORTACI√ìN DE 'direl_ts_tool_kit'
# ======================================================


def parse_datetime_index(df, date_column="date", format="%d/%m/%Y %H:%M:%S"):
    """
    Convierte una columna de fecha a datetime y la usa como √≠ndice
    (Reemplaza a la funci√≥n de direl_ts_tool_kit)
    """
    df_copy = df.copy()
    df_copy[date_column] = pd.to_datetime(
        df_copy[date_column], format=format, errors="coerce"
    )
    df_copy.set_index(date_column, inplace=True)
    return df_copy


def plot_time_series_with_forecast(df_historico_completo, df_pronostico, variable, units="", time_unit="Day", pasos_pronostico=72):
    """
    Grafica una serie de tiempo con datos hist√≥ricos COMPLETOS (1200 datos) y pron√≥stico
    """
    fig, ax = plt.subplots(figsize=(16, 7))
    
    # Obtener los √∫ltimos N datos hist√≥ricos para mostrar (aproximadamente 50 d√≠as)
    datos_a_mostrar = min(len(df_historico_completo), 1200)  # Mostrar hasta 1200 puntos
    
    if datos_a_mostrar < len(df_historico_completo):
        df_historico_a_mostrar = df_historico_completo.tail(datos_a_mostrar)
    else:
        df_historico_a_mostrar = df_historico_completo.copy()
    
    # Plot datos hist√≥ricos COMPLETOS (1200 datos)
    ax.plot(df_historico_a_mostrar.index, df_historico_a_mostrar[variable], 
            linewidth=1.5, color='blue', alpha=0.7, label=f'Hist√≥rico ({len(df_historico_a_mostrar)} datos)')
    
    # Marcar el punto donde empieza el pron√≥stico
    ultima_fecha_historica = df_historico_a_mostrar.index[-1]
    ultimo_valor_historico = df_historico_a_mostrar[variable].iloc[-1]
    
    # Plot datos de pron√≥stico (72 horas)
    if df_pronostico is not None and not df_pronostico.empty:
        ax.plot(df_pronostico.index, df_pronostico['pronostico'], 
                linewidth=2, color='red', label='Pron√≥stico 72h')
        
        # Sombrear intervalo de confianza 80%
        ax.fill_between(df_pronostico.index, 
                        df_pronostico['limite_inferior_80'], 
                        df_pronostico['limite_superior_80'],
                        alpha=0.2, color='orange', label='Intervalo confianza 80%')
        
        # Sombrear intervalo de confianza 95%
        ax.fill_between(df_pronostico.index, 
                        df_pronostico['limite_inferior_95'], 
                        df_pronostico['limite_superior_95'],
                        alpha=0.1, color='gray', label='Intervalo confianza 95%')
        
        # Marcar transici√≥n hist√≥rico-pron√≥stico
        ax.scatter([ultima_fecha_historica], [ultimo_valor_historico], 
                  color='green', s=100, zorder=5, 
                  label=f'√öltimo dato: {ultima_fecha_historica.strftime("%d/%m %H:%M")}')
    
    ax.set_title(f"{variable} - Hist√≥rico (1200 datos) + Pron√≥stico 72h", fontsize=14, fontweight='bold')
    ax.set_xlabel("Fecha y Hora")
    ax.set_ylabel(f"{variable} ({units})")
    ax.grid(True, alpha=0.3)
    ax.legend(loc='best', fontsize=10)

    # Formato de fechas
    ax.xaxis.set_major_formatter(mdates.DateFormatter("%d/%m %H:%M"))
    plt.setp(ax.xaxis.get_majorticklabels(), rotation=45)
    
    # Agregar informaci√≥n en el gr√°fico
    info_text = f"Datos hist√≥ricos: {len(df_historico_a_mostrar)} registros\n"
    info_text += f"Pron√≥stico: {pasos_pronostico} horas\n"
    info_text += f"Rango hist√≥rico: {df_historico_a_mostrar.index[0].strftime('%d/%m %H:%M')} a {ultima_fecha_historica.strftime('%d/%m %H:%M')}"
    
    plt.figtext(0.02, 0.02, info_text, fontsize=9, 
                bbox=dict(boxstyle="round,pad=0.3", facecolor="lightyellow", alpha=0.8))

    plt.tight_layout()
    return fig

# ======================================================
# 6. CONECTAR Y CARGAR DATOS
# ======================================================

print("\n" + "=" * 60)
print("üì• CARGANDO DATOS DESDE GOOGLE SHEETS")
print("=" * 60)

# Conectar a Google Sheets
worksheet = conectar_a_google_sheets()

if worksheet is not None:
    # Obtener todos los datos
    datos = worksheet.get_all_values()

    # Convertir a DataFrame
    df0 = pd.DataFrame(datos[1:], columns=datos[0])

    print(f"‚úÖ Datos cargados desde Google Sheets")
    print(f"üìä Dimensiones: {df0.shape[0]} filas √ó {df0.shape[1]} columnas")
    print(f"üîó URL: {SHEET_URL}")

else:
    # ========== DATOS DE EJEMPLO (si falla la conexi√≥n) ==========
    print("üìù Usando datos de ejemplo para demostraci√≥n...")

    # Crear datos de ejemplo que incluyan 2025
    import numpy as np
    
    # Crear fechas desde 2025 hasta ahora
    fechas_2025 = pd.date_range(start="2025-01-01", end="2025-12-31", freq="10min")
    fechas_2026 = pd.date_range(start="2026-01-01", end="2026-02-02", freq="10min")
    fechas = list(fechas_2025) + list(fechas_2026)
    
    # Crear datos sint√©ticos con tendencia y estacionalidad
    n = len(fechas)
    t = np.linspace(0, 20, n)
    
    datos_ejemplo = {
        "Date": [f.strftime("%d/%m/%Y %H:%M:%S") for f in fechas],
        "Temperature": 25 + 5 * np.sin(t/24) + 0.1 * np.sin(t/168) + 0.5 * np.random.randn(n),
        "Humidity": 60 + 10 * np.cos(t/12) + 2 * np.cos(t/84) + 1 * np.random.randn(n),
        "PM 2.5(¬µg/m¬≥)": 20 + 8 * np.sin(t/48) + 3 * np.random.randn(n),
        "PM 10 (¬µg/m¬≥)": 40 + 12 * np.cos(t/36) + 4 * np.random.randn(n),
        "PM 1.0 (¬µg/m¬≥)": 10 + 4 * np.sin(t/60) + 2 * np.random.randn(n),
    }

    df0 = pd.DataFrame(datos_ejemplo)
    print(f"üìä Datos de ejemplo creados: {df0.shape[0]} filas")
    print(f"üìÖ Rango de fechas: {df0['Date'].iloc[0]} a {df0['Date'].iloc[-1]}")

print("\nüìã Primeras filas de datos:")
print(df0.head(2))
print("\nüìã √öltimas filas de datos:")
print(df0.tail(2))

# ======================================================
# 6.1. VERIFICACI√ìN DE ACTUALIZACI√ìN DE DATOS
# ======================================================
print("\nüìÖ Verificando actualizaci√≥n de datos...")
print(f"√öltima fecha en datos: {df0['Date'].iloc[-1]}")
print(f"Total de registros: {len(df0)}")

# Verificar si hay datos recientes
try:
    ultima_fecha_str = df0['Date'].iloc[-1]
    ultima_fecha = pd.to_datetime(ultima_fecha_str, format="%d/%m/%Y %H:%M:%S", errors='coerce')
    if ultima_fecha:
        hoy = datetime.now()
        diferencia = hoy - ultima_fecha
        print(f"D√≠as desde el √∫ltimo registro: {diferencia.days}")
        
        if diferencia.days > 7:
            print("‚ö†Ô∏è  ADVERTENCIA: Los datos pueden estar desactualizados")
        else:
            print("‚úÖ Datos actualizados recientemente")
except Exception as e:
    print(f"‚ö†Ô∏è  Error verificando actualizaci√≥n: {e}")

df0.rename(
    columns={
        "Date": "date",
        "PM 1.0 (¬µg/m¬≥)": "PM 1",
        "PM 2.5(¬µg/m¬≥)": "PM 2.5",
        "PM 10 (¬µg/m¬≥)": "PM 10",
    },
    inplace=True,
)

# ======================================================
# CONVERSI√ìN DE DATOS Y LIMPIEZA
# ======================================================

print("üîç Verificando tipos de datos iniciales:")
print(df0.dtypes)

# Identificar columnas que deber√≠an ser num√©ricas
columnas_numericas = ["Temperature", "Humidity", "PM 2.5", "PM 10", "PM 1"]

# Convertir cada columna a num√©rico, forzando errores a NaN
for col in columnas_numericas:
    if col in df0.columns:
        print(f"\nConvirtiendo columna '{col}'...")
        
        # Guardar valores originales para comparar
        valores_originales = df0[col].head(5).tolist()
        
        # Convertir a num√©rico
        df0[col] = pd.to_numeric(df0[col], errors='coerce')
        
        # Contar valores convertidos y no convertidos
        total_valores = len(df0[col])
        valores_nan = df0[col].isna().sum()
        valores_convertidos = total_valores - valores_nan
        
        print(f"  Valores originales (primeros 5): {valores_originales}")
        print(f"  Valores convertidos: {valores_convertidos}/{total_valores}")
        print(f"  Valores no convertidos (NaN): {valores_nan}")

# Crear DataFrame con √≠ndice temporal
df1 = parse_datetime_index(df0, format="%d/%m/%Y %H:%M:%S")

print("\n‚úÖ Tipos de datos despu√©s de la conversi√≥n:")
print(df1.dtypes)

# ======================================================
# LIMPIEZA ADICIONAL ANTES DEL RESAMPLE (SOLUCI√ìN AL ERROR)
# ======================================================

print("\nüßπ Realizando limpieza adicional antes del resample...")

# 1. Eliminar columnas vac√≠as o con nombres vac√≠os
df1 = df1.loc[:, ~df1.columns.astype(str).str.contains('^Unnamed')]
df1 = df1.loc[:, ~df1.columns.astype(str).str.contains('^$')]

# 2. Seleccionar solo columnas num√©ricas conocidas
columnas_a_mantener = ['Temperature', 'Humidity', 'PM 1', 'PM 2.5', 'PM 10']
columnas_existentes = [col for col in columnas_a_mantener if col in df1.columns]

print(f"üìã Columnas a mantener: {columnas_existentes}")

# 3. Verificar que todas las columnas sean num√©ricas
for col in columnas_existentes:
    if df1[col].dtype not in ['int64', 'float64']:
        print(f"‚ö†Ô∏è  Columna '{col}' no es num√©rica, convirtiendo...")
        df1[col] = pd.to_numeric(df1[col], errors='coerce')

# 4. Eliminar filas donde todas las columnas sean NaN
df1 = df1.dropna(how='all')

print(f"\nüìä DataFrame despu√©s de limpieza:")
print(f"  - Forma: {df1.shape[0]} filas √ó {df1.shape[1]} columnas")
print(f"  - Tipos de datos: {df1.dtypes.to_dict()}")

# ======================================================
# RESAMPLE CON SEGURIDAD
# ======================================================

print("\nüìä Realizando resample a 10 minutos...")
try:
    # Opci√≥n 1: Usar numeric_only=True para seguridad
    df1 = df1.resample("10min").mean(numeric_only=True)
    print("‚úÖ Resample completado exitosamente con numeric_only=True")
    
except Exception as e:
    print(f"‚ö†Ô∏è  Error con numeric_only=True: {e}")
    print("üîÑ Intentando m√©todo alternativo...")
    
    # Opci√≥n 2: M√©todo alternativo manual
    try:
        # Crear lista para almacenar resultados
        resample_data = []
        
        # Para cada columna num√©rica
        for col in df1.columns:
            if df1[col].dtype in ['int64', 'float64']:
                # Resample por columna
                col_resampled = df1[col].resample("10min").mean()
                resample_data.append(col_resampled)
        
        # Combinar resultados
        df1 = pd.concat(resample_data, axis=1)
        print("‚úÖ Resample completado exitosamente con m√©todo alternativo")
        
    except Exception as e2:
        print(f"‚ùå Error en m√©todo alternativo: {e2}")
        print("‚ö†Ô∏è  Continuando sin resample...")
        # Mantener df1 sin cambios

print(f"üìä Nueva forma despu√©s del resample: {df1.shape[0]} filas √ó {df1.shape[1]} columnas")

# Mostrar estad√≠sticas
print("\nüìà Estad√≠sticas descriptivas:")
print(df1.describe().transpose())

# ======================================================
# GUARDAR DATOS COMPLETOS PARA GR√ÅFICAS FUTURAS
# ======================================================
# Guardar una copia de todos los datos para mostrar en gr√°ficas
df_completo_para_graficas = df1.copy()
print(f"\nüìä Datos completos guardados para gr√°ficas: {len(df_completo_para_graficas)} registros")
print(f"üìÖ Rango completo: {df_completo_para_graficas.index[0]} a {df_completo_para_graficas.index[-1]}")

# ======================================================
# IMPUTACI√ìN DE VALORES FALTANTES
# ======================================================
df2 = df1.copy()

vars_to_impute = ["Temperature", "Humidity", "PM 10", "PM 2.5"]

for var in vars_to_impute:
    # Marcar NaN antes de imputar
    df2[f"{var}_imputed"] = df2[var].isna()

    # Calcular medias por hora:minuto
    means = df2.groupby(df2.index.time)[var].transform("mean")

    # Llenar SOLO los NaN, sin alterar valores originales
    df2[var] = df2[var].fillna(means)

df2 = df2.loc[:, ~df2.columns.str.endswith("_imputed")]

# ======================================================
# MODIFICACI√ìN CR√çTICA: SELECCI√ìN DE √öLTIMOS 1200 DATOS Y ELIMINACI√ìN DE √öLTIMOS 2 D√çAS
# ======================================================
print("\n" + "="*80)
print("üìä SELECCI√ìN DE DATOS PARA MODELADO SARIMA")
print("="*80)

# Copiar el DataFrame
df3 = df2.copy()

print(f"üìä Datos totales disponibles: {len(df3)} registros")

# ======================================================
# PASO 1: SELECCIONAR √öLTIMOS 1200 DATOS
# ======================================================

# Obtener los √∫ltimos 1200 datos
total_datos = len(df3)
datos_a_usar = min(1200, total_datos)

# Seleccionar los √∫ltimos N datos
df3_temp = df3.tail(datos_a_usar).copy()

print(f"\n‚úÖ √öltimos {len(df3_temp)} datos seleccionados para procesar")
print(f"üìÖ Rango inicial seleccionado: {df3_temp.index[0]} a {df3_temp.index[-1]}")
print(f"üìÖ D√≠as cubiertos: {(df3_temp.index[-1] - df3_temp.index[0]).days} d√≠as")

# ======================================================
# PASO 2: ELIMINAR LOS √öLTIMOS 2 D√çAS DE ESOS 1200 DATOS
# ======================================================

# Encontrar las fechas √∫nicas en los datos seleccionados
fechas_unicas = sorted(set(df3_temp.index.date))
print(f"\nüìÖ Fechas √∫nicas en los 1200 datos seleccionados: {len(fechas_unicas)} d√≠as")

if len(fechas_unicas) > 2:
    # Seleccionar los DOS √∫ltimos d√≠as
    dias_a_eliminar = fechas_unicas[-2:]  # Los dos √∫ltimos d√≠as
    
    print(f"\nüóëÔ∏è  Eliminando datos de los √∫ltimos 2 d√≠as:")
    for dia in dias_a_eliminar:
        registros_dia = sum(df3_temp.index.date == dia)
        print(f"   - {dia.strftime('%Y-%m-%d')}: {registros_dia} registros")
    
    # Convertir a lista de strings para comparaci√≥n
    dias_a_eliminar_str = [d.strftime("%Y-%m-%d") for d in dias_a_eliminar]
    
    # Crear una m√°scara booleana
    mask = df3_temp.index.strftime("%Y-%m-%d").isin(dias_a_eliminar_str)
    
    # Contar cu√°ntos registros se eliminar√°n
    registros_a_eliminar = mask.sum()
    print(f"\n   Total de registros a eliminar: {registros_a_eliminar}")
    
    # Eliminar los dos √∫ltimos d√≠as
    df3 = df3_temp[~mask].copy()
    
    print(f"\n‚úÖ Datos despu√©s de eliminar √∫ltimos 2 d√≠as:")
    print(f"   - Registros restantes: {len(df3)}")
    print(f"   - Rango final: {df3.index[0]} a {df3.index[-1]}")
    print(f"   - D√≠as cubiertos: {(df3.index[-1] - df3.index[0]).days} d√≠as")
    
else:
    # Si hay 2 d√≠as o menos, usar todos los datos disponibles
    print(f"\n‚ö†Ô∏è  Solo hay {len(fechas_unicas)} d√≠as de datos, usando todos disponibles")
    df3 = df3_temp.copy()

# ======================================================
# PASO 3: ELIMINAR COLUMNA PM 1 (si existe)
# ======================================================
df3 = df3.drop(columns=["PM 1"], errors="ignore")

# ======================================================
# PASO 4: GUARDAR DATOS HIST√ìRICOS PARA GR√ÅFICAS
# ======================================================
# df3 contiene los √∫ltimos 1200 datos SIN los √∫ltimos 2 d√≠as
df_historico_para_modelo = df3.copy()

print(f"\nüìã RESUMEN FINAL PARA MODELADO SARIMA:")
print(f"   - Total registros para entrenar modelo: {len(df_historico_para_modelo)}")
print(f"   - Columnas: {list(df_historico_para_modelo.columns)}")
print(f"   - Rango temporal para modelo: {df_historico_para_modelo.index[0]} a {df_historico_para_modelo.index[-1]}")

# ======================================================
# CONFIGURAR CARPETA DE SALIDA SIMPLIFICADA
# ======================================================
print("\n" + "=" * 60)
print("üìÅ CONFIGURANDO CARPETA DE SALIDA (pronosticos/)")
print("=" * 60)

carpeta_pronosticos = configurar_carpeta_salida()

# ======================================================
# 1. RESAMPLEO A DATOS POR HORA CON FRECUENCIA EXPL√çCITA
# ======================================================
# Asegurar que el √≠ndice tenga frecuencia para SARIMA
df_hourly = df_historico_para_modelo.resample("H").mean().dropna()

# Asignar frecuencia expl√≠cita al √≠ndice (IMPORTANTE PARA SARIMA)
if not df_hourly.index.freq:
    print("‚ö†Ô∏è  Asignando frecuencia horaria al √≠ndice...")
    df_hourly = df_hourly.asfreq('H')

variables = ["Temperature", "Humidity", "PM 2.5", "PM 10"]

print(f"\nüìä Datos horarios para modelado SARIMA:")
print(f"   - Registros: {len(df_hourly)}")
print(f"   - Rango: {df_hourly.index[0]} a {df_hourly.index[-1]}")
print(f"   - Variables: {variables}")

# ======================================================
# 2. OPTIMIZADOR SARIMA (SIN PMDARIMA)
# ======================================================
p = d = q = [0, 1]
P = D = Q = [0, 1]
m = 12  # estacionalidad de 12 horas


def buscar_mejor_modelo(series):
    mejor_aic = float("inf")
    mejor_modelo = None
    mejor_orden = None
    mejor_orden_season = None
    mejores_parametros = None
    mejor_summary = None

    for pi in p:
        for di in d:
            for qi in q:
                for Pi in P:
                    for Di in D:
                        for Qi in Q:
                            orden = (pi, di, qi)
                            orden_seas = (Pi, Di, Qi, m)

                            try:
                                modelo = SARIMAX(
                                    series,
                                    order=orden,
                                    seasonal_order=orden_seas,
                                    enforce_stationarity=False,
                                    enforce_invertibility=False,
                                ).fit(disp=False, maxiter=200)

                                if modelo.aic < mejor_aic:
                                    mejor_aic = modelo.aic
                                    mejor_modelo = modelo
                                    mejor_orden = orden
                                    mejor_orden_season = orden_seas
                                    mejores_parametros = modelo.params
                                    mejor_summary = modelo.summary()

                            except:
                                continue

    return (
        mejor_modelo,
        mejor_orden,
        mejor_orden_season,
        mejor_aic,
        mejores_parametros,
        mejor_summary,
    )

# ======================================================
# 3. FUNCI√ìN PARA OBTENER ECUACI√ìN MATEM√ÅTICA
# ======================================================
def obtener_ecuacion_sarima(modelo, orden, orden_seas):
    """
    Genera la ecuaci√≥n matem√°tica del modelo SARIMA
    """
    p, d, q = orden
    P, D, Q, m = orden_seas

    # Obtener par√°metros
    params = modelo.params

    # Inicializar partes de la ecuaci√≥n
    parte_ar = ""
    parte_ma = ""
    parte_sar = ""
    parte_sma = ""

    # Coeficientes AR (no estacional)
    for i in range(1, p + 1):
        if f"ar.L{i}" in params:
            coef = params[f"ar.L{i}"]
            parte_ar += f" + {coef:.4f}¬∑y_t-{i}"

    # Coeficientes MA (no estacional)
    for i in range(1, q + 1):
        if f"ma.L{i}" in params:
            coef = params[f"ma.L{i}"]
            parte_ma += f" + {coef:.4f}¬∑Œµ_t-{i}"

    # Coeficientes SAR (estacional)
    for i in range(1, P + 1):
        if f"ar.S.L{m*i}" in params:
            coef = params[f"ar.S.L{m*i}"]
            parte_sar += f" + {coef:.4f}¬∑y_t-{m*i}"

    # Coeficientes SMA (estacional)
    for i in range(1, Q + 1):
        if f"ma.S.L{m*i}" in params:
            coef = params[f"ma.S.L{m*i}"]
            parte_sma += f" + {coef:.4f}¬∑Œµ_t-{m*i}"

    # Constante
    constante = ""
    if "intercept" in params:
        constante = f"{params['intercept']:.4f} + "
    elif "const" in params:
        constante = f"{params['const']:.4f} + "

    # Construir ecuaci√≥n
    if d == 0 and D == 0:
        ecuacion = f"y_t = {constante}"
    else:
        # Para modelos con diferenciaci√≥n
        ecuacion = "Œî^d Œî_s^D y_t = "
        if constante.strip():
            ecuacion = f"Œî^d Œî_s^D y_t = {constante}"

    # Agregar partes
    if parte_ar:
        ecuacion += parte_ar[3:] if ecuacion.endswith("= ") else parte_ar
    if parte_ma:
        ecuacion += parte_ma
    if parte_sar:
        ecuacion += parte_sar
    if parte_sma:
        ecuacion += parte_sma

    if parte_ar or parte_ma or parte_sar or parte_sma:
        ecuacion += " + Œµ_t"
    else:
        ecuacion += "Œµ_t"

    return ecuacion

# ======================================================
# 4. FUNCI√ìN CORREGIDA PARA GUARDAR PRON√ìSTICO COMO JSON
# CON TODOS LOS DATOS HIST√ìRICOS (1200 datos - 2 d√≠as)
# ======================================================
def guardar_pronostico_json(modelo, serie_hourly, df_historico_original, variable, pasos=72):
    """Guarda el pron√≥stico completo en un archivo JSON con datos hist√≥ricos COMPLETOS"""
    try:
        if carpeta_pronosticos is None:
            print("‚ö†Ô∏è  No se pudo crear carpeta, no se guardar√° el pron√≥stico")
            return None
        
        # Obtener pron√≥stico de 72 horas
        pred = modelo.get_forecast(steps=pasos)
        media = pred.predicted_mean
        conf_80 = pred.conf_int(alpha=0.20)
        conf_95 = pred.conf_int(alpha=0.05)
        
        # Obtener √∫ltima fecha hist√≥rica
        ultima_fecha_historica = serie_hourly.index[-1]
        
        # Crear fechas para el pron√≥stico (72 horas)
        fechas_pronostico = pd.date_range(
            start=ultima_fecha_historica + pd.Timedelta(hours=1), 
            periods=pasos, 
            freq='H'
        )
        
        # Preparar datos del pron√≥stico (TODAS LAS 72 HORAS)
        pronostico_data = []
        for i in range(pasos):
            pronostico_data.append({
                'fecha': fechas_pronostico[i].strftime('%Y-%m-%d %H:%M:%S'),
                'pronostico': float(media.values[i]),
                'limite_inferior_80': float(conf_80.iloc[i, 0]),
                'limite_superior_80': float(conf_80.iloc[i, 1]),
                'limite_inferior_95': float(conf_95.iloc[i, 0]),
                'limite_superior_95': float(conf_95.iloc[i, 1])
            })
        
        # Preparar datos hist√≥ricos COMPLETOS (1200 datos - 2 d√≠as)
        # Convertir df_historico_original a resoluci√≥n horaria para consistencia
        df_historico_hourly = df_historico_original.resample("H").mean()
        
        datos_historicos_completos = []
        for i, (fecha, valor) in enumerate(zip(df_historico_hourly.index, df_historico_hourly[variable].values)):
            datos_historicos_completos.append({
                'fecha': fecha.strftime('%Y-%m-%d %H:%M:%S'),
                'valor_real': float(valor)
            })
        
        # Informaci√≥n del modelo
        orden = modelo.specification.order
        orden_seas = modelo.specification.seasonal_order
        
        # Obtener ecuaci√≥n
        ecuacion = obtener_ecuacion_sarima(modelo, orden, orden_seas)
        
        # Crear estructura completa del JSON
        datos_completos = {
            'metadata': {
                'variable': variable,
                'modelo': f'SARIMA{orden}{orden_seas}',
                'fecha_entrenamiento': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                'datos_historicos': {
                    'total_registros': len(df_historico_hourly),
                    'rango_temporal': {
                        'inicio': df_historico_hourly.index[0].strftime('%Y-%m-%d %H:%M:%S'),
                        'fin': df_historico_hourly.index[-1].strftime('%Y-%m-%d %H:%M:%S')
                    },
                    'descripcion': f'√öltimos 1200 datos del Google Sheets, eliminando √∫ltimos 2 d√≠as. Total: {len(df_historico_hourly)} registros horarios.'
                },
                'fecha_ultimo_dato_historico': ultima_fecha_historica.strftime('%Y-%m-%d %H:%M:%S'),
                'valor_ultimo_dato': float(serie_hourly.iloc[-1]),
                'horizonte_pronostico': pasos,
                'unidades': get_units(variable),
                'ecuacion': ecuacion,
                'metricas': {
                    'aic': float(modelo.aic),
                    'bic': float(modelo.bic),
                    'hqic': float(modelo.hqic),
                    'log_likelihood': float(modelo.llf),
                    'num_observaciones': int(modelo.nobs)
                }
            },
            'parametros': {str(k): float(v) for k, v in modelo.params.items()},
            'datos_historicos_completos': datos_historicos_completos,  # TODOS los datos hist√≥ricos
            'pronostico': pronostico_data  # TODAS LAS 72 HORAS
        }
        
        # Guardar en JSON
        nombre_archivo = f"pronostico_{variable}.json"
        ruta_completa = os.path.join(carpeta_pronosticos, nombre_archivo)
        
        with open(ruta_completa, 'w', encoding='utf-8') as f:
            json.dump(datos_completos, f, indent=2, ensure_ascii=False)
        
        print(f"‚úÖ Pron√≥stico JSON guardado en: {ruta_completa}")
        print(f"   - {len(datos_historicos_completos)} datos hist√≥ricos completos")
        print(f"   - {pasos} horas de pron√≥stico")
        
        return ruta_completa, pronostico_data
        
    except Exception as e:
        print(f"‚ö†Ô∏è  Error guardando pron√≥stico JSON para {variable}: {e}")
        import traceback
        traceback.print_exc()
        return None, None

def get_units(variable):
    """Obtiene las unidades para cada variable"""
    units = {
        "Temperature": "¬∞C",
        "Humidity": "%",
        "PM 2.5": "¬µg/m¬≥",
        "PM 10": "¬µg/m¬≥"
    }
    return units.get(variable, "")

# ======================================================
# 5. EJECUCI√ìN COMPLETA POR VARIABLE CON MANEJO DE ERRORES
# ======================================================
resultados = {}
ecuaciones = {}
archivos_guardados = {}
pronosticos_data = {}
todos_los_historicos = {}  # Para almacenar datos hist√≥ricos de cada variable

print(f"\n{'='*80}")
print("üöÄ INICIANDO PROCESO DE MODELADO SARIMA")
print(f"{'='*80}")

for var in variables:
    print(f"\n{'='*80}")
    print(f" üìà PROCESANDO VARIABLE: {var}")
    print(f"{'='*80}")

    serie = df_hourly[var]

    try:
        modelo, orden, orden_s, aic, parametros, summary = buscar_mejor_modelo(serie)

        print(f"\n‚úÖ Mejor modelo encontrado para {var}: SARIMA{orden}{orden_s}")
        print(f"üìä AIC = {aic:.2f}")

        # Obtener y mostrar ecuaci√≥n matem√°tica
        try:
            ecuacion = obtener_ecuacion_sarima(modelo, orden, orden_s)
            print(f"\nüßÆ Ecuaci√≥n matem√°tica:")
            print(f"{ecuacion}")
        except Exception as e:
            print(f"‚ö†Ô∏è  Error obteniendo ecuaci√≥n: {e}")
            ecuacion = "No disponible"

        # Guardar resultados en diccionarios
        resultados[var] = modelo
        ecuaciones[var] = ecuacion
        
        # Guardar pron√≥stico como JSON con datos hist√≥ricos COMPLETOS
        try:
            print(f"\nüíæ Guardando JSON con datos hist√≥ricos completos...")
            ruta_json, datos_pronostico = guardar_pronostico_json(
                modelo, 
                serie, 
                df_historico_para_modelo,  # Datos hist√≥ricos completos (1200 - 2 d√≠as)
                var, 
                pasos=72
            )
            if ruta_json:
                archivos_guardados[var] = ruta_json
                pronosticos_data[var] = datos_pronostico
                
                # Guardar datos hist√≥ricos para graficar
                todos_los_historicos[var] = df_historico_para_modelo[var].copy()
                
        except Exception as e:
            print(f"‚ö†Ô∏è  Error guardando JSON para {var}: {e}")
            pronosticos_data[var] = None
            
    except Exception as e:
        print(f"‚ùå Error procesando {var}: {e}")
        print(f"‚ö†Ô∏è  Saltando variable {var}...")

# ======================================================
# 6. GRAFICAR DATOS HIST√ìRICOS COMPLETOS + PRON√ìSTICO 72 HORAS
# ======================================================
print(f"\n{'='*80}")
print("üìà GENERANDO GR√ÅFICAS CON HIST√ìRICO COMPLETO Y PRON√ìSTICO")
print(f"{'='*80}")

for var in variables:
    if var in resultados and var in pronosticos_data and pronosticos_data[var] is not None:
        try:
            # Preparar DataFrame de pron√≥stico para graficar
            df_pronostico = pd.DataFrame(pronosticos_data[var])
            
            # Convertir fecha a datetime
            df_pronostico['fecha'] = pd.to_datetime(df_pronostico['fecha'])
            df_pronostico.set_index('fecha', inplace=True)
            
            # Obtener datos hist√≥ricos COMPLETOS para esta variable
            # df_historico_para_modelo ya tiene los 1200 datos - 2 d√≠as
            df_historico_completo = df_historico_para_modelo.copy()
            
            # Verificar que tenemos datos hist√≥ricos
            if var in df_historico_completo.columns and len(df_historico_completo) > 0:
                print(f"\nüìä Generando gr√°fica para {var}:")
                print(f"   - Datos hist√≥ricos: {len(df_historico_completo)} registros")
                print(f"   - Rango hist√≥rico: {df_historico_completo.index[0]} a {df_historico_completo.index[-1]}")
                print(f"   - Datos de pron√≥stico: {len(df_pronostico)} horas")
                
                # Graficar
                fig = plot_time_series_with_forecast(
                    df_historico_completo=df_historico_completo,
                    df_pronostico=df_pronostico,
                    variable=var,
                    units=get_units(var),
                    time_unit="Day",
                    pasos_pronostico=72
                )
                
                plt.show()
                
                # Tambi√©n guardar la gr√°fica
                if carpeta_pronosticos:
                    nombre_grafica = f"grafica_{var}.png"
                    ruta_grafica = os.path.join(carpeta_pronosticos, nombre_grafica)
                    fig.savefig(ruta_grafica, dpi=150, bbox_inches='tight')
                    print(f"   ‚úÖ Gr√°fica guardada: {ruta_grafica}")
                    
            else:
                print(f"‚ö†Ô∏è  No hay datos hist√≥ricos para graficar {var}")
                
        except Exception as e:
            print(f"‚ö†Ô∏è  Error generando gr√°fica para {var}: {e}")
            import traceback
            traceback.print_exc()
    else:
        print(f"‚ö†Ô∏è  No se pudo generar gr√°fica para {var}: faltan datos de modelo o pron√≥stico")

# ======================================================
# 7. RESUMEN FINAL DE TODOS LOS MODELOS
# ======================================================
print(f"\n{'='*80}")
print(" üìä RESUMEN FINAL DE MODELOS SARIMA")
print(f"{'='*80}")

for var in variables:
    if var in resultados:
        modelo = resultados[var]
        print(f"\nüìà {var}:")
        print(f"   Modelo: SARIMA{modelo.specification.order}")
        print(f"           {modelo.specification.seasonal_order}")
        print(f"   AIC: {modelo.aic:.2f}")
        print(f"   Datos hist√≥ricos para entrenamiento: {len(df_historico_para_modelo)} registros")
        print(f"   √öltimo dato hist√≥rico: {df_historico_para_modelo[var].iloc[-1]:.2f} {get_units(var)}")
        print(f"   Fecha √∫ltimo dato: {df_historico_para_modelo.index[-1].strftime('%Y-%m-%d %H:%M')}")
        print(f"   Horas pronosticadas: 72")
    else:
        print(f"\n‚ùå {var}: No se pudo ajustar modelo")

# ======================================================
# 8. RESUMEN DE ARCHIVOS GUARDADOS
# ======================================================
print(f"\n{'='*80}")
print(" üíæ ARCHIVOS GUARDADOS EN pronosticos/")
print(f"{'='*80}")

if archivos_guardados:
    for variable, ruta in archivos_guardados.items():
        print(f"  ‚úÖ JSON - {variable}: {os.path.basename(ruta)}")
    
    print(f"\nüìä Total de archivos JSON generados: {len(archivos_guardados)}")
    
    # Mostrar contenido real de la carpeta
    if carpeta_pronosticos and os.path.exists(carpeta_pronosticos):
        archivos_en_carpeta = os.listdir(carpeta_pronosticos)
        archivos_json = [f for f in archivos_en_carpeta if f.endswith('.json')]
        archivos_png = [f for f in archivos_en_carpeta if f.endswith('.png')]
        
        print(f"\nüìÇ Contenido actual de {carpeta_pronosticos}:")
        
        print("  üìÑ Archivos JSON:")
        for archivo in archivos_json:
            ruta_completa = os.path.join(carpeta_pronosticos, archivo)
            tama√±o = os.path.getsize(ruta_completa)
            tama√±o_kb = tama√±o / 1024
            
            # Verificar contenido
            with open(ruta_completa, 'r', encoding='utf-8') as f:
                datos = json.load(f)
                horas_pronostico = len(datos.get('pronostico', []))
                datos_historicos = len(datos.get('datos_historicos_completos', []))
            
            print(f"    - {archivo} ({tama√±o_kb:.1f} KB)")
            print(f"      ‚Üí {datos_historicos} datos hist√≥ricos, {horas_pronostico} horas pron√≥stico")
        
        if archivos_png:
            print(f"\n  üñºÔ∏è  Archivos PNG (gr√°ficas):")
            for archivo in archivos_png:
                ruta_completa = os.path.join(carpeta_pronosticos, archivo)
                tama√±o = os.path.getsize(ruta_completa)
                tama√±o_kb = tama√±o / 1024
                print(f"    - {archivo} ({tama√±o_kb:.1f} KB)")
else:
    print("  üì≠ No se guardaron archivos.")

# ======================================================
# 9. RESUMEN DE PROCESAMIENTO DE DATOS
# ======================================================
print(f"\n{'='*80}")
print(" üîÑ RESUMEN DE PROCESAMIENTO DE DATOS")
print(f"{'='*80}")

print(f"\nüìä Datos cargados de Google Sheets:")
print(f"  - Total registros originales: {len(df0)}")
print(f"  - Primera fecha: {df0['date'].iloc[0]}")
print(f"  - √öltima fecha: {df0['date'].iloc[-1]}")

print(f"\nüìä Procesamiento para modelado SARIMA:")
print(f"  - √öltimos datos tomados: 1200 registros")
print(f"  - D√≠as eliminados: √∫ltimos 2 d√≠as")
print(f"  - Datos para entrenamiento: {len(df_historico_para_modelo)} registros")
print(f"  - Rango temporal para modelo: {df_historico_para_modelo.index[0]} a {df_historico_para_modelo.index[-1]}")

print(f"\nüìä Datos horarios para SARIMA:")
print(f"  - Registros horarios: {len(df_hourly)}")
print(f"  - Rango horario: {df_hourly.index[0]} a {df_hourly.index[-1]}")

# ======================================================
# 10. VERIFICACI√ìN DE CONTENIDO DE JSON
# ======================================================
print(f"\n{'='*80}")
print(" ‚úÖ VERIFICACI√ìN DE CONTENIDO EN ARCHIVOS JSON")
print(f"{'='*80}")

for var in variables:
    if var in archivos_guardados and archivos_guardados[var]:
        try:
            with open(archivos_guardados[var], 'r', encoding='utf-8') as f:
                datos = json.load(f)
                horas_pronostico = len(datos.get('pronostico', []))
                datos_historicos = len(datos.get('datos_historicos_completos', []))
                descripcion = datos.get('metadata', {}).get('datos_historicos', {}).get('descripcion', '')
                
                print(f"\nüìã {var}:")
                print(f"  - Datos hist√≥ricos en JSON: {datos_historicos} registros")
                print(f"  - Horas de pron√≥stico: {horas_pronostico}")
                print(f"  - Descripci√≥n: {descripcion}")
                
                if datos_historicos > 100 and horas_pronostico == 72:
                    print(f"  ‚úÖ VERIFICACI√ìN CORRECTA: Datos completos y pron√≥stico de 72h")
                else:
                    print(f"  ‚ö†Ô∏è  VERIFICACI√ìN INCOMPLETA: Esperados >100 datos hist√≥ricos y 72h pron√≥stico")
                    
        except Exception as e:
            print(f"‚ùå Error leyendo JSON de {var}: {e}")

print(f"\n{'='*80}")
print(" üéØ PROCESO COMPLETADO EXITOSAMENTE")
print(f"{'='*80}")
print("    ‚Ä¢ Gr√°ficas con √∫ltimos 1200 datos (sin √∫ltimos 2 d√≠as)")
print("    ‚Ä¢ Gr√°ficas con pron√≥stico de 72 horas")
print("    ‚Ä¢ JSON con datos hist√≥ricos COMPLETOS")
print("    ‚Ä¢ JSON con 72 horas de pron√≥stico")
print(f"{'='*80}")
