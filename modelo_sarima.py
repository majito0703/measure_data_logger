# -*- coding: utf-8 -*-
"""Modelo_Sarima.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iAFMXztN6AvkjVaC3_b2wtbi-jGTxS8R
"""



# -*- coding: utf-8 -*-
"""Modelo_Sarima.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iAFMXztN6AvkjVaC3_b2wtbi-jGTxS8R
"""

import os
import pickle
import json
import warnings

warnings.filterwarnings("ignore")

# ======================================================
# URL DE GOOGLE SHEETS (definida a nivel global)
# ======================================================
SHEET_URL = "https://docs.google.com/spreadsheets/d/1x1FeUolFWlR07tgrc6F4cgeUhJYV7uQ5yuRTBHO8jWI/edit?gid=0#gid=0"

# ======================================================
# 1. FUNCI√ìN DE CONEXI√ìN A GOOGLE SHEETS (COLAB + GITHUB)
# ======================================================


def conectar_a_google_sheets():
    """
    Conecta a Google Sheets de manera inteligente
    - En Colab: usa autenticaci√≥n normal
    - En GitHub: puede usar Service Account
    """

    try:
        # Verificar si estamos en Google Colab
        try:
            from google.colab import auth

            IN_COLAB = True
        except:
            IN_COLAB = False

        if IN_COLAB:
            # ========== MODO COLAB ==========
            print("üîë Autenticando en Google Colab...")

            # Autenticaci√≥n interactiva
            auth.authenticate_user()
            from google.auth import default

            creds, _ = default()

            # Guardar credenciales (opcional)
            with open("/content/token.pickle", "wb") as token:
                pickle.dump(creds, token)

            import gspread

            gc = gspread.authorize(creds)

        else:
            # ========== MODO GITHUB/LOCAL ==========
            import gspread

            # Intentar con variable de entorno (para GitHub Actions)
            creds_json = os.getenv("GOOGLE_SHEETS_CREDS")

            if creds_json:
                print("üîë Usando Service Account desde variable de entorno...")
                from google.oauth2.service_account import Credentials

                creds_dict = json.loads(creds_json)
                scope = ["https://www.googleapis.com/auth/spreadsheets"]
                credentials = Credentials.from_service_account_info(
                    creds_dict, scopes=scope
                )
                gc = gspread.authorize(credentials)
            else:
                # Intentar autenticaci√≥n normal (fallback)
                print("‚ö†Ô∏è  Intentando autenticaci√≥n normal...")
                gc = gspread.oauth()  # Esto abrir√° navegador en local

        # Abrir la hoja
        spreadsheet = gc.open_by_url(SHEET_URL)
        worksheet = spreadsheet.get_worksheet(0)

        print("‚úÖ Conexi√≥n exitosa a Google Sheets")
        return worksheet

    except Exception as e:
        print(f"‚ùå Error conectando a Google Sheets: {e}")
        print("‚ö†Ô∏è  Usando datos de ejemplo para continuar...")
        return None


# ======================================================
# 2. CONFIGURACI√ìN INICIAL DE COLAB
# ======================================================

# Montar Google Drive (solo funciona en Colab)
try:
    from google.colab import drive

    drive.mount("/content/drive", force_remount=False)
    IN_COLAB = True

    # Crear carpeta para pron√≥sticos
    os.makedirs("/content/drive/MyDrive/pronosticos", exist_ok=True)
    print("‚úÖ Google Drive montado y carpeta creada")

except:
    IN_COLAB = False
    print("‚ö†Ô∏è  No es Google Colab, omitiendo montaje de Drive")

# ======================================================
# 3. INSTALAR DEPENDENCIAS (SOLO COLAB)
# ======================================================

if IN_COLAB:
    print("üì¶ Instalando dependencias en Colab...")
    !python3 -m pip install --upgrade pip >& /dev/null
    !python3 -m pip install jupyter-black >& /dev/null
    !pip install gspread pandas numpy matplotlib statsmodels >& /dev/null
else:
    print("‚úÖ En GitHub, las dependencias se instalan desde requirements.txt")

# ======================================================
# 4. IMPORTAR BIBLIOTECAS
# ======================================================

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import gspread
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
from statsmodels.tsa.statespace.sarimax import SARIMAX
import matplotlib.dates as mdates
from datetime import timedelta

# Cargar jupyter-black (solo Colab)
if IN_COLAB:
    import jupyter_black

    jupyter_black.load()

# ======================================================
# 5. REEMPLAZAR LA IMPORTACI√ìN DE 'direl_ts_tool_kit'
# ======================================================


def parse_datetime_index(df, date_column="date", format="%d/%m/%Y %H:%M:%S"):
    """
    Convierte una columna de fecha a datetime y la usa como √≠ndice
    (Reemplaza a la funci√≥n de direl_ts_tool_kit)
    """
    df_copy = df.copy()
    df_copy[date_column] = pd.to_datetime(
        df_copy[date_column], format=format, errors="coerce"
    )
    df_copy.set_index(date_column, inplace=True)
    return df_copy


def plot_time_series(df, variable, units="", time_unit="Day"):
    """
    Grafica una serie de tiempo
    (Versi√≥n simplificada de la funci√≥n original)
    """
    fig, ax = plt.subplots(figsize=(12, 5))

    ax.plot(df.index, df[variable], linewidth=1)
    ax.set_title(f"Serie de Tiempo - {variable}")
    ax.set_xlabel(f"Tiempo ({time_unit})")
    ax.set_ylabel(f"{variable} {units}")
    ax.grid(True, alpha=0.3)

    # Formato de fechas
    ax.xaxis.set_major_formatter(mdates.DateFormatter("%d/%m %H:%M"))
    plt.setp(ax.xaxis.get_majorticklabels(), rotation=45)

    plt.tight_layout()
    return fig


# ======================================================
# 6. CONECTAR Y CARGAR DATOS
# ======================================================

print("\n" + "=" * 60)
print("üì• CARGANDO DATOS DESDE GOOGLE SHEETS")
print("=" * 60)

# Conectar a Google Sheets
worksheet = conectar_a_google_sheets()

if worksheet is not None:
    # Obtener todos los datos
    datos = worksheet.get_all_values()

    # Convertir a DataFrame
    df0 = pd.DataFrame(datos[1:], columns=datos[0])

    print(f"‚úÖ Datos cargados desde Google Sheets")
    print(f"üìä Dimensiones: {df0.shape[0]} filas √ó {df0.shape[1]} columnas")
    print(f"üîó URL: {SHEET_URL}")

else:
    # ========== DATOS DE EJEMPLO (si falla la conexi√≥n) ==========
    print("üìù Usando datos de ejemplo para demostraci√≥n...")

    # Crear datos de ejemplo simples
    import numpy as np

    fechas = pd.date_range(start="2024-01-01", periods=100, freq="H")

    datos_ejemplo = {
        "Date": [f.strftime("%d/%m/%Y %H:%M:%S") for f in fechas],
        "Temperature": 25 + 5 * np.sin(np.linspace(0, 10, 100)),
        "Humidity": 60 + 10 * np.cos(np.linspace(0, 8, 100)),
        "PM 2.5(¬µg/m¬≥)": 20 + 8 * np.random.randn(100),
        "PM 10 (¬µg/m¬≥)": 40 + 12 * np.random.randn(100),
        "PM 1.0 (¬µg/m¬≥)": 10 + 4 * np.random.randn(100),
    }

    df0 = pd.DataFrame(datos_ejemplo)
    print(f"üìä Datos de ejemplo creados: {df0.shape[0]} filas")

print("\nüìã Primeras filas de datos:")
print(df0.head(2))

df0.head(2)

df0.rename(
    columns={
        "Date": "date",
        "PM 1.0 (¬µg/m¬≥)": "PM 1",
        "PM 2.5(¬µg/m¬≥)": "PM 2.5",
        "PM 10 (¬µg/m¬≥)": "PM 10",
    },
    inplace=True,
)

df0["Temperature"] = pd.to_numeric(df0["Temperature"], errors="coerce")
df0["Humidity"] = pd.to_numeric(df0["Humidity"], errors="coerce")
df0["PM 2.5"] = pd.to_numeric(df0["PM 2.5"], errors="coerce")
df0["PM 10"] = pd.to_numeric(df0["PM 10"], errors="coerce")

df1 = parse_datetime_index(df0, format="%d/%m/%Y %H:%M:%S")
df1.head(10)

df1 = parse_datetime_index(df0, format="%d/%m/%Y %H:%M:%S")
df1.head(10)

# ======================================================
# LIMPIAR Y CONVERTIR DATOS ANTES DE RESAMPLE
# ======================================================
print("üîç Verificando tipos de datos antes de resample:")
print(df1.dtypes)

# Identificar columnas que deber√≠an ser num√©ricas
columnas_numericas = ["Temperature", "Humidity", "PM 2.5", "PM 10", "PM 1"]

# Convertir cada columna a num√©rico, forzando errores a NaN
for col in columnas_numericas:
    if col in df1.columns:
        print(f"\nConvirtiendo columna '{col}'...")

        # Guardar valores originales para comparar
        valores_originales = df1[col].head(5).tolist()

        # Convertir a num√©rico
        df1[col] = pd.to_numeric(df1[col], errors="coerce")

        # Contar valores convertidos y no convertidos
        total_valores = len(df1[col])
        valores_nan = df1[col].isna().sum()
        valores_convertidos = total_valores - valores_nan

        print(f"  Valores originales (primeros 5): {valores_originales}")
        print(f"  Valores convertidos: {valores_convertidos}/{total_valores}")
        print(f"  Valores no convertidos (NaN): {valores_nan}")

        # Mostrar ejemplos de valores no convertidos
        if valores_nan > 0:
            valores_problematicos = df0.loc[df1[col].isna(), col].unique()[:5]
            print(f"  Valores problem√°ticos encontrados: {valores_problematicos}")

# Verificar tipos despu√©s de la conversi√≥n
print("\n‚úÖ Tipos de datos despu√©s de la conversi√≥n:")
print(df1.dtypes)

# Ahora s√≠ hacer el resample
print("\nüìä Realizando resample a 10 minutos...")
df1 = df1.resample("10min").mean()

print("‚úÖ Resample completado exitosamente")
print(f"üìä Nueva forma: {df1.shape[0]} filas √ó {df1.shape[1]} columnas")

# Mostrar estad√≠sticas
print("\nüìà Estad√≠sticas descriptivas:")
print(df1.describe().transpose())

fig = plot_time_series(df1, variable="Temperature", units="¬∞C", time_unit="Day")
plt.show()

fig = plot_time_series(df1, variable="Humidity", units="%", time_unit="Day")
plt.show()

fig = plot_time_series(df1, variable="PM 10", units="¬µg/m¬≥", time_unit="Day")
plt.show()

fig = plot_time_series(df1, variable="PM 2.5", units="¬µg/m¬≥", time_unit="Day")
plt.show()

df2 = df1.copy()

vars_to_impute = ["Temperature", "Humidity", "PM 10", "PM 2.5"]

for var in vars_to_impute:
    # Marcar NaN antes de imputar
    df2[f"{var}_imputed"] = df2[var].isna()

    # Calcular medias por hora:minuto
    means = df2.groupby(df2.index.time)[var].transform("mean")

    # Llenar SOLO los NaN, sin alterar valores originales
    df2[var] = df2[var].fillna(means)

df2 = df2.loc[:, ~df2.columns.str.endswith("_imputed")]
# Ver primeras filas del nuevo DataFrame
print(df2.head())

# Ver solo los datos imputados de cada variable
print(df2[[col for col in df2.columns if "imputed" in col]].sum())
df2 = df2.loc[:, ~df2.columns.str.endswith("_imputed")]

fig = plot_time_series(df2, variable="Temperature", units="¬∞C", time_unit="Day")
plt.show()

fig = plot_time_series(df2, variable="Humidity", units="%", time_unit="Day")
plt.show()

fig = plot_time_series(df2, variable="PM 10", units="¬µg/m¬≥", time_unit="Day")
plt.show()

fig = plot_time_series(df2, variable="PM 2.5", units="¬µg/m¬≥", time_unit="Day")
plt.show()

df3 = df2.copy()
# Eliminar los d√≠as 24 y 25
df3 = df3[~df3.index.day.isin([24, 25])]
df3 = df3.drop(columns=["PM 1"])
# Verificar los primeros registros
print(df3.head())

# Verificar los √∫ltimos registros para asegurarte que se eliminaron los d√≠as 19 y 20
print(df3.tail())

# ======================================================
# 1. RESAMPLEO A DATOS POR HORA
# ======================================================
df_hourly = df3.resample("H").mean().dropna()

variables = ["Temperature", "Humidity", "PM 2.5", "PM 10"]

# ======================================================
# 2. OPTIMIZADOR SARIMA (SIN PMDARIMA)
# ======================================================
p = d = q = [0, 1]
P = D = Q = [0, 1]
m = 12  # estacionalidad de 12 horas


def buscar_mejor_modelo(series):
    mejor_aic = float("inf")
    mejor_modelo = None
    mejor_orden = None
    mejor_orden_season = None
    mejores_parametros = None
    mejor_summary = None

    for pi in p:
        for di in d:
            for qi in q:
                for Pi in P:
                    for Di in D:
                        for Qi in Q:
                            orden = (pi, di, qi)
                            orden_seas = (Pi, Di, Qi, m)

                            try:
                                modelo = SARIMAX(
                                    series,
                                    order=orden,
                                    seasonal_order=orden_seas,
                                    enforce_stationarity=False,
                                    enforce_invertibility=False,
                                ).fit(disp=False, maxiter=200)

                                if modelo.aic < mejor_aic:
                                    mejor_aic = modelo.aic
                                    mejor_modelo = modelo
                                    mejor_orden = orden
                                    mejor_orden_season = orden_seas
                                    mejores_parametros = modelo.params
                                    mejor_summary = modelo.summary()

                            except:
                                continue

    return (
        mejor_modelo,
        mejor_orden,
        mejor_orden_season,
        mejor_aic,
        mejores_parametros,
        mejor_summary,
    )

# ======================================================
# 3. FUNCI√ìN PARA OBTENER ECUACI√ìN MATEM√ÅTICA
# ======================================================
def obtener_ecuacion_sarima(modelo, orden, orden_seas):
    """
    Genera la ecuaci√≥n matem√°tica del modelo SARIMA
    """
    p, d, q = orden
    P, D, Q, m = orden_seas

    # Obtener par√°metros
    params = modelo.params

    # Inicializar partes de la ecuaci√≥n
    parte_ar = ""
    parte_ma = ""
    parte_sar = ""
    parte_sma = ""

    # Coeficientes AR (no estacional)
    for i in range(1, p + 1):
        if f"ar.L{i}" in params:
            coef = params[f"ar.L{i}"]
            parte_ar += f" + {coef:.4f}¬∑y_t-{i}"

    # Coeficientes MA (no estacional)
    for i in range(1, q + 1):
        if f"ma.L{i}" in params:
            coef = params[f"ma.L{i}"]
            parte_ma += f" + {coef:.4f}¬∑Œµ_t-{i}"

    # Coeficientes SAR (estacional)
    for i in range(1, P + 1):
        if f"ar.S.L{m*i}" in params:
            coef = params[f"ar.S.L{m*i}"]
            parte_sar += f" + {coef:.4f}¬∑y_t-{m*i}"

    # Coeficientes SMA (estacional)
    for i in range(1, Q + 1):
        if f"ma.S.L{m*i}" in params:
            coef = params[f"ma.S.L{m*i}"]
            parte_sma += f" + {coef:.4f}¬∑Œµ_t-{m*i}"

    # Constante
    constante = ""
    if "intercept" in params:
        constante = f"{params['intercept']:.4f} + "
    elif "const" in params:
        constante = f"{params['const']:.4f} + "

    # Construir ecuaci√≥n
    if d == 0 and D == 0:
        ecuacion = f"y_t = {constante}"
    else:
        # Para modelos con diferenciaci√≥n
        ecuacion = "Œî^d Œî_s^D y_t = "
        if constante.strip():
            ecuacion = f"Œî^d Œî_s^D y_t = {constante}"

    # Agregar partes
    if parte_ar:
        ecuacion += parte_ar[3:] if ecuacion.endswith("= ") else parte_ar
    if parte_ma:
        ecuacion += parte_ma
    if parte_sar:
        ecuacion += parte_sar
    if parte_sma:
        ecuacion += parte_sma

    if parte_ar or parte_ma or parte_sar or parte_sma:
        ecuacion += " + Œµ_t"
    else:
        ecuacion += "Œµ_t"

    return ecuacion

# ======================================================
# 4. FUNCI√ìN PARA MOSTRAR PAR√ÅMETROS COMO EN LA IMAGEN
# ======================================================
def mostrar_parametros_tabla(modelo, orden, orden_seas, aic):
    """
    Muestra los par√°metros en formato de tabla como en la imagen
    """
    params = modelo.params
    p, d, q = orden
    P, D, Q, m = orden_seas

    print("\n" + "=" * 60)
    print("PAR√ÅMETROS DEL MODELO SARIMA")
    print("=" * 60)

    # Crear lista de par√°metros
    parametros_lista = []

    # Par√°metros AR
    for i in range(1, p + 1):
        key = f"ar.L{i}"
        if key in params:
            parametros_lista.append((f"œÜ_{i}", params[key], modelo.bse.get(key, "N/A")))

    # Par√°metros MA
    for i in range(1, q + 1):
        key = f"ma.L{i}"
        if key in params:
            parametros_lista.append((f"Œ∏_{i}", params[key], modelo.bse.get(key, "N/A")))

    # Par√°metros SAR
    for i in range(1, P + 1):
        key = f"ar.S.L{m*i}"
        if key in params:
            parametros_lista.append((f"Œ¶_{i}", params[key], modelo.bse.get(key, "N/A")))

    # Par√°metros SMA
    for i in range(1, Q + 1):
        key = f"ma.S.L{m*i}"
        if key in params:
            parametros_lista.append((f"Œò_{i}", params[key], modelo.bse.get(key, "N/A")))

    # Constante
    if "intercept" in params:
        parametros_lista.append(
            ("intercept", params["intercept"], modelo.bse.get("intercept", "N/A"))
        )
    elif "const" in params:
        parametros_lista.append(
            ("const", params["const"], modelo.bse.get("const", "N/A"))
        )

    # Varianza del error
    if "sigma2" in params:
        parametros_lista.append(
            ("œÉ¬≤", params["sigma2"], modelo.bse.get("sigma2", "N/A"))
        )

    # Mostrar tabla
    print(f"\nOrden: SARIMA{orden}{orden_seas}")
    print(f"AIC: {aic:.2f}")
    print("\n" + "-" * 60)
    print(f"{'Par√°metro':<15} {'Valor':<15} {'Error est√°ndar':<15}")
    print("-" * 60)

    for nombre, valor, error in parametros_lista:
        if isinstance(error, (int, float)):
            print(f"{nombre:<15} {valor:<15.4f} {error:<15.4f}")
        else:
            print(f"{nombre:<15} {valor:<15.4f} {str(error):<15}")

    print("-" * 60)

# ======================================================
# 5. FUNCI√ìN PARA GRAFICAR PRON√ìSTICO CON FECHAS MEJORADAS
# ======================================================
def graficar_pronostico(modelo, series, pasos=48, limite=None):
    pred = modelo.get_forecast(steps=pasos)
    media = pred.predicted_mean
    conf_80 = pred.conf_int(alpha=0.20)
    conf_95 = pred.conf_int(alpha=0.05)

    # Crear figura con tama√±o adecuado
    fig, ax = plt.subplots(figsize=(15, 6))

    # Datos hist√≥ricos
    ax.plot(series.index, series.values, label="Medido", color="black", linewidth=1.5)

    # Pron√≥stico
    ax.plot(
        media.index,
        media.values,
        label=f"Pron√≥stico {series.name}",
        color="red",
        linewidth=2,
    )

    # Bandas de confianza
    ax.fill_between(
        conf_80.index,
        conf_80.iloc[:, 0],
        conf_80.iloc[:, 1],
        color="green",
        alpha=0.3,
        label="Confianza 80%",
    )
    ax.fill_between(
        conf_95.index,
        conf_95.iloc[:, 0],
        conf_95.iloc[:, 1],
        color="yellow",
        alpha=0.2,
        label="Confianza 95%",
    )

    # L√≠nea vertical para separar historial de pron√≥stico
    ultimo_historial = series.index[-1]
    ax.axvline(x=ultimo_historial, color="gray", linestyle="--", alpha=0.7, linewidth=1)

    # L√≠nea de norma o l√≠mite permitido
    if limite is not None:
        ax.axhline(
            y=limite,
            color="blue",
            linestyle="--",
            linewidth=2,
            label="Nivel m√°ximo permitido (24H) en Colombia",
        )

    # Configurar formato de fechas
    # Determinar el rango de fechas
    fecha_min = series.index.min()
    fecha_max = media.index.max()

    # Calcular diferencia de d√≠as para determinar el formato
    dias_totales = (fecha_max - fecha_min).days

    if dias_totales <= 7:  # Si es menos de una semana
        # Formato: D√≠a Hora (ej: "11 Nov 10:00")
        ax.xaxis.set_major_formatter(mdates.DateFormatter("%d %b %H:%M"))
        ax.xaxis.set_major_locator(mdates.HourLocator(interval=6))
    elif dias_totales <= 30:  # Si es menos de un mes
        # Formato: D√≠a Mes (ej: "11 Nov")
        ax.xaxis.set_major_formatter(mdates.DateFormatter("%d %b"))
        ax.xaxis.set_major_locator(mdates.DayLocator(interval=2))
    else:  # Si es m√°s de un mes
        # Formato: Mes (ej: "Nov 2025")
        ax.xaxis.set_major_formatter(mdates.DateFormatter("%b %Y"))
        ax.xaxis.set_major_locator(mdates.MonthLocator())

    # Rotar etiquetas para mejor lectura
    plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha="right")

    # Agregar leyenda de separaci√≥n
    ax.text(
        ultimo_historial + timedelta(hours=1),
        ax.get_ylim()[1] * 0.95,
        "Pron√≥stico",
        fontsize=10,
        color="darkred",
        alpha=0.8,
    )

    # T√≠tulos y etiquetas
    ax.set_title(f"Pron√≥stico SARIMA - {series.name}", fontsize=14, fontweight="bold")
    ax.set_xlabel("Fecha y Hora", fontsize=12)

    # Etiqueta del eje Y dependiendo de la variable
    if series.name == "Temperature":
        ax.set_ylabel("Temperatura (¬∞C)", fontsize=12)
    elif series.name == "Humidity":
        ax.set_ylabel("Humedad (%)", fontsize=12)
    elif series.name in ["PM 2.5", "PM 10"]:
        ax.set_ylabel(f"{series.name} (¬µg/m¬≥)", fontsize=12)
    else:
        ax.set_ylabel(series.name, fontsize=12)

    # Cuadr√≠cula
    ax.grid(True, alpha=0.3, linestyle="--")

    # Leyenda
    ax.legend(loc="upper left", fontsize=10)

    # Ajustar m√°rgenes
    plt.tight_layout()

    plt.show()

    # Guardar la figura en Google Drive
    nombre_archivo = (
        f"pronostico_{series.name}_{pd.Timestamp.now().strftime('%Y%m%d_%H%M')}.png"
    )
    ruta_drive = f"/content/drive/MyDrive/pronosticos/{nombre_archivo}"
    fig.savefig(ruta_drive, dpi=150, bbox_inches="tight")

    # Tambi√©n guardar los datos del pron√≥stico
    pronostico_data = {
        "fecha": media.index.strftime("%Y-%m-%d %H:%M:%S").tolist(),
        "pronostico": media.values.tolist(),
        "limite_inferior_80": conf_80.iloc[:, 0].tolist(),
        "limite_superior_80": conf_80.iloc[:, 1].tolist(),
        "limite_inferior_95": conf_95.iloc[:, 0].tolist(),
        "limite_superior_95": conf_95.iloc[:, 1].tolist(),
    }

    df_pronostico = pd.DataFrame(pronostico_data)
    df_pronostico.to_csv(
        f"/content/drive/MyDrive/pronosticos/datos_{series.name}.csv", index=False
    )

    return nombre_archivo  # Esto es nuevo

# ======================================================
# 6. EJECUCI√ìN COMPLETA POR VARIABLE
# ======================================================
limites = {
    "Temperature": None,
    "Humidity": None,
    "PM 2.5": 37,  # valor OMS o Colombia
    "PM 10": 75,
}

resultados = {}
ecuaciones = {}
parametros_tablas = {}

for var in variables:
    print(f"\n{'='*80}")
    print(f" OPTIMIZANDO: {var}")
    print(f"{'='*80}")

    serie = df_hourly[var]

    modelo, orden, orden_s, aic, parametros, summary = buscar_mejor_modelo(serie)

    print(f"\nMejor modelo para {var}: SARIMA{orden}{orden_s}")
    print(f"AIC = {aic:.2f}")

    # Obtener y mostrar ecuaci√≥n matem√°tica
    ecuacion = obtener_ecuacion_sarima(modelo, orden, orden_s)
    print(f"\nEcuaci√≥n matem√°tica:")
    print(f"{ecuacion}")

    # Mostrar par√°metros en formato de tabla
    mostrar_parametros_tabla(modelo, orden, orden_s, aic)

    # Guardar resultados
    resultados[var] = modelo
    ecuaciones[var] = ecuacion
    parametros_tablas[var] = {
        "orden": orden,
        "orden_estacional": orden_s,
        "aic": aic,
        "parametros": parametros,
        "summary": summary,
    }

    # Graficar pron√≥stico (72 horas adelante)
    graficar_pronostico(modelo, serie, pasos=72, limite=limites[var])

# ======================================================
# 7. RESUMEN FINAL DE TODOS LOS MODELOS
# ======================================================
print(f"\n{'='*80}")
print(" RESUMEN FINAL DE MODELOS SARIMA")
print(f"{'='*80}")

for var in variables:
    print(f"\n{var}:")
    print(f"  Modelo: SARIMA{resultados[var].specification.order}")
    print(f"          {resultados[var].specification.seasonal_order}")
    print(f"  AIC: {resultados[var].aic:.2f}")
    print(f"  Ecuaci√≥n: {ecuaciones[var]}")
    print(f"  N√∫mero de observaciones: {len(df_hourly[var])}")

# ======================================================
# 9. SUBIR DATOS COMPLETOS A GITHUB (TODO EL HISTORIAL) - VERSI√ìN CORREGIDA
# ======================================================

print("\n" + "=" * 80)
print(" üì§ SUBIENDO DATOS COMPLETOS A GITHUB")
print("=" * 80)

# 1. CONFIGURAR GITHUB
import requests
import base64
import json
from datetime import datetime
import pandas as pd

# üî¥ REEMPLAZA ESTO CON TU TOKEN DE GITHUB (si es necesario)
GITHUB_TOKEN = "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"
GITHUB_USER = "majito0703"
REPO_NAME = "measure_data_logger"
BRANCH = "main"


# 2. FUNCI√ìN PARA CREAR ARCHIVOS JSON CON TODO EL HISTORIAL - CORREGIDA
def crear_archivos_json_completos():
    print("\nüìù Creando archivos JSON con TODOS los datos...")

    json_files = {}

    for var in variables:
        modelo = resultados[var]
        serie = df_hourly[var]

        print(f"  üìä Procesando {var}...")
        print(f"    Total de datos hist√≥ricos: {len(serie)} registros")

        # Obtener pron√≥stico
        pasos = 72  # 72 horas
        pred = modelo.get_forecast(steps=pasos)
        media = pred.predicted_mean
        conf_80 = pred.conf_int(alpha=0.20)
        conf_95 = pred.conf_int(alpha=0.05)

        # üö®üö®üö® DATOS HIST√ìRICOS COMPLETOS üö®üö®üö®
        historico = []
        for fecha, valor in serie.items():
            historico.append(
                {"fecha": fecha.strftime("%Y-%m-%d %H:%M:%S"), "valor": float(valor)}
            )

        print(f"    Hist√≥ricos procesados: {len(historico)} registros")
        if historico:
            print(f"    Primer dato: {historico[0]['fecha']}")
            print(f"    √öltimo dato: {historico[-1]['fecha']}")
        else:
            print("    ‚ö†Ô∏è  No hay datos hist√≥ricos")

        # üö®üö®üö® DATOS DE PRON√ìSTICO COMPLETOS üö®üö®üö®
        pronosticos = []
        for i in range(len(media)):
            pronosticos.append(
                {
                    "fecha": media.index[i].strftime("%Y-%m-%d %H:%M:%S"),
                    "pronostico": float(media.iloc[i]),
                    "confianza_80_min": float(conf_80.iloc[i, 0]),
                    "confianza_80_max": float(conf_80.iloc[i, 1]),
                    "confianza_95_min": float(conf_95.iloc[i, 0]),
                    "confianza_95_max": float(conf_95.iloc[i, 1]),
                }
            )

        print(f"    Pron√≥sticos generados: {len(pronosticos)} horas")

        # Informaci√≥n del modelo
        info_modelo = {
            "variable": var,
            "nombre_display": {
                "Temperature": "Temperatura",
                "Humidity": "Humedad",
                "PM 2.5": "PM2.5",
                "PM 10": "PM10",
            }.get(var, var),
            "unidad": (
                "¬∞C" if var == "Temperature" else "%" if var == "Humidity" else "¬µg/m¬≥"
            ),
            "modelo": f"SARIMA{resultados[var].specification.order}{resultados[var].specification.seasonal_order}",
            "aic": float(resultados[var].aic),
            "observaciones_totales": len(serie),
            "ecuacion": ecuaciones[var],
            "limite_permitido": limites.get(var),
            "parametros": {
                "orden": resultados[var].specification.order,
                "orden_estacional": resultados[var].specification.seasonal_order,
                "coeficientes": {
                    k: float(v) for k, v in resultados[var].params.items()
                },
            },
        }

        # üö®üö®üö® DATOS ESTAD√çSTICOS COMPLETOS üö®üö®üö®
        estadisticas = {}
        if len(serie) > 0:
            estadisticas = {
                "min": float(serie.min()),
                "max": float(serie.max()),
                "media": float(serie.mean()),
                "mediana": float(serie.median()),
                "desviacion_estandar": float(serie.std()),
                "primer_registro": serie.index[0].strftime("%Y-%m-%d %H:%M:%S"),
                "ultimo_registro": serie.index[-1].strftime("%Y-%m-%d %H:%M:%S"),
                "rango_dias": (serie.index[-1] - serie.index[0]).days,
            }

        # üö®üö®üö® CREAR ESTRUCTURA COMPLETA - USAR "historico" (NO "historial_completo") üö®üö®üö®
        datos_completos = {
            **info_modelo,
            "fecha_generacion": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "estadisticas": estadisticas,
            "historico": historico,  # üö® CAMBIADO DE "historial_completo" a "historico"
            "pronosticos": pronosticos,
            "total_pronosticos": len(pronosticos),
            "metadata": {
                "generado_por": "modelo_sarima.py",
                "version": "2.0",
                "ultima_actualizacion": datetime.now().isoformat(),
                "resample_frecuencia": "1H",
                "datos_fuente": "Google Sheets",
            },
        }

        # üö®üö®üö® USAR NOMBRES DE ARCHIVO CONSISTENTES üö®üö®üö®
        # Temperatura -> temperature.json
        # Humedad -> humidity.json
        # PM 2.5 -> pm2.5.json
        # PM 10 -> pm10.json

        nombre_base = var.lower().replace(" ", "").replace(".", "").replace("pm", "pm")
        if nombre_base == "temperature":
            nombre_base = "temperature"
        elif nombre_base == "humidity":
            nombre_base = "humidity"
        elif nombre_base == "pm25":
            nombre_base = "pm2.5"  # Usar pm2.5 con punto
        elif nombre_base == "pm10":
            nombre_base = "pm10"

        nombre_json = f"pronostico_{nombre_base}.json"

        with open(nombre_json, "w", encoding="utf-8") as f:
            json.dump(datos_completos, f, ensure_ascii=False, indent=2)

        # Verificar tama√±o
        tama√±o_kb = len(json.dumps(datos_completos, ensure_ascii=False)) / 1024
        print(
            f"    ‚úÖ {nombre_json} ({tama√±o_kb:.1f} KB, {len(historico)} registros hist√≥ricos)"
        )

        json_files[nombre_json] = datos_completos

    return json_files


# 3. FUNCI√ìN PARA SUBIR A GITHUB - MEJORADA CON MEJOR MANEJO DE ERRORES
def subir_a_github_json(nombre_archivo, datos_json, mensaje_commit):
    url = f"https://api.github.com/repos/{GITHUB_USER}/{REPO_NAME}/contents/pronosticos/{nombre_archivo}"

    headers = {
        "Authorization": f"token {GITHUB_TOKEN}",
        "Accept": "application/vnd.github.v3+json",
        "User-Agent": "Colab-Auto-Upload",
    }

    try:
        # Convertir datos JSON a base64
        contenido_str = json.dumps(datos_json, ensure_ascii=False, indent=2)
        contenido_bytes = contenido_str.encode("utf-8")
        contenido_base64 = base64.b64encode(contenido_bytes).decode("utf-8")

        # Verificar si el archivo ya existe
        sha = None
        try:
            response = requests.get(url, headers=headers, timeout=10)
            if response.status_code == 200:
                sha = response.json().get("sha")
                print(f"    üìÑ Archivo existe, SHA: {sha[:8]}...")
            elif response.status_code == 404:
                print(f"    üìÑ Archivo nuevo")
            else:
                print(f"    ‚ö†Ô∏è  Error HTTP {response.status_code} al verificar")
                return False
        except Exception as e:
            print(f"    ‚ö†Ô∏è  Error de conexi√≥n al verificar: {str(e)[:100]}")

        # Preparar datos para subir
        data = {
            "message": mensaje_commit,
            "content": contenido_base64,
            "branch": BRANCH,
        }

        if sha:
            data["sha"] = sha

        # Subir archivo
        response = requests.put(url, headers=headers, json=data, timeout=30)

        if response.status_code in [200, 201]:
            print(f"    ‚úÖ Subido exitosamente")
            return True
        else:
            try:
                error_data = response.json()
                error_msg = error_data.get("message", "Sin mensaje")
                print(f"    ‚ùå Error {response.status_code}: {error_msg}")
            except:
                print(f"    ‚ùå Error {response.status_code}: {response.text[:200]}")
            return False

    except Exception as e:
        print(f"    ‚ùå Error en subida: {str(e)}")
        return False


# 4. üö®üö®üö® CREAR Y SUBIR ARCHIVOS JSON CON TODOS LOS DATOS üö®üö®üö®
print("\nüìä Generando datos COMPLETOS de pron√≥stico...")
json_files = crear_archivos_json_completos()

print("\nüì§ Subiendo archivos JSON a GitHub...")
fecha = datetime.now().strftime("%Y-%m-%d %H:%M")
mensaje_commit = f"ü§ñ Actualizaci√≥n de pron√≥sticos SARIMA - {fecha}"

exitosos = 0
total_archivos = len(json_files)

for i, (archivo_json, datos_json) in enumerate(json_files.items(), 1):
    try:
        print(f"\n  [{i}/{total_archivos}] üìÑ Subiendo: {archivo_json}")

        # Mostrar informaci√≥n del archivo
        var = datos_json["variable"]
        total_historial = len(datos_json.get("historico", []))
        total_pronosticos = len(datos_json.get("pronosticos", []))

        print(
            f"    üìä {var}: {total_historial} datos hist√≥ricos + {total_pronosticos} pron√≥sticos"
        )

        if total_historial > 0:
            primer_dato = datos_json["historico"][0]["fecha"]
            ultimo_historico = datos_json["historico"][-1]["fecha"]
            print(f"    üìÖ Hist√≥rico: {primer_dato} ‚Üí {ultimo_historico}")

        if total_pronosticos > 0:
            primer_pronostico = datos_json["pronosticos"][0]["fecha"]
            ultimo_pronostico = datos_json["pronosticos"][-1]["fecha"]
            print(f"    üîÆ Pron√≥stico: {primer_pronostico} ‚Üí {ultimo_pronostico}")

        # Subir a GitHub
        if subir_a_github_json(archivo_json, datos_json, mensaje_commit):
            exitosos += 1
        else:
            print(f"    ‚ö†Ô∏è  Fall√≥ la subida, continuando...")

    except Exception as e:
        print(f"    ‚ùå Error procesando {archivo_json}: {str(e)}")

# 5. CREAR ARCHIVO CSV COMPLEMENTARIO (PARA F√ÅCIL AN√ÅLISIS)
print("\nüìù Creando archivos CSV complementarios...")

for var in variables:
    try:
        # Crear DataFrame con todos los datos hist√≥ricos
        serie = df_hourly[var]
        df_historial = pd.DataFrame({"fecha": serie.index, "valor": serie.values})
        df_historial["fecha"] = df_historial["fecha"].dt.strftime("%Y-%m-%d %H:%M:%S")

        # Determinar nombre de archivo consistente
        nombre_base = var.lower().replace(" ", "").replace(".", "").replace("pm", "pm")
        if nombre_base == "temperature":
            nombre_base = "temperature"
        elif nombre_base == "humidity":
            nombre_base = "humidity"
        elif nombre_base == "pm25":
            nombre_base = "pm2.5"
        elif nombre_base == "pm10":
            nombre_base = "pm10"

        csv_nombre = f"datos_{nombre_base}.csv"
        df_historial.to_csv(csv_nombre, index=False, encoding="utf-8")

        print(f"  ‚úÖ {var}: {csv_nombre} ({len(df_historial)} registros)")

        # Tambi√©n subir CSV a GitHub
        with open(csv_nombre, "r", encoding="utf-8") as f:
            contenido = f.read()
            contenido_bytes = contenido.encode("utf-8")
            contenido_base64 = base64.b64encode(contenido_bytes).decode("utf-8")

        csv_github_nombre = f"pronosticos/datos_{nombre_base}.csv"
        if subir_a_github_json(
            f"datos_{nombre_base}.csv", {"contenido": contenido}, mensaje_commit
        ):
            exitosos += 1

    except Exception as e:
        print(f"  ‚ö†Ô∏è  Error creando CSV para {var}: {str(e)}")

# 6. CREAR ARCHIVO √çNDICE CON METADATOS COMPLETOS
print("\nüìã Creando archivo √≠ndice con metadatos...")

indice_pronosticos = {
    "fecha_actualizacion": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
    "total_variables": len(variables),
    "pronosticos_disponibles": [],
    "estadisticas_generales": {},
    "endpoints": {},
}

for var in variables:
    # Determinar nombre base consistente
    nombre_base = var.lower().replace(" ", "").replace(".", "").replace("pm", "pm")
    if nombre_base == "temperature":
        nombre_base = "temperature"
    elif nombre_base == "humidity":
        nombre_base = "humidity"
    elif nombre_base == "pm25":
        nombre_base = "pm2.5"
    elif nombre_base == "pm10":
        nombre_base = "pm10"

    serie = df_hourly[var]

    indice_pronosticos["pronosticos_disponibles"].append(
        {
            "variable": var,
            "nombre_mostrar": {
                "Temperature": "Temperatura",
                "Humidity": "Humedad",
                "PM 2.5": "PM2.5",
                "PM 10": "PM10",
            }.get(var, var),
            "archivo_json": f"pronostico_{nombre_base}.json",
            "archivo_csv": f"datos_{nombre_base}.csv",
            "unidad": (
                "¬∞C" if var == "Temperature" else "%" if var == "Humidity" else "¬µg/m¬≥"
            ),
            "limite": limites.get(var),
            "estadisticas": {
                "total_datos": len(serie),
                "primer_dato": serie.index[0].strftime("%Y-%m-%d %H:%M:%S"),
                "ultimo_dato": serie.index[-1].strftime("%Y-%m-%d %H:%M:%S"),
                "min": float(serie.min()),
                "max": float(serie.max()),
                "media": float(serie.mean()),
            },
        }
    )

    indice_pronosticos["endpoints"][nombre_base] = {
        "json": f"https://raw.githubusercontent.com/{GITHUB_USER}/{REPO_NAME}/main/pronosticos/pronostico_{nombre_base}.json",
        "csv": f"https://raw.githubusercontent.com/{GITHUB_USER}/{REPO_NAME}/main/pronosticos/datos_{nombre_base}.csv",
        "raw_json": f"https://raw.githubusercontent.com/{GITHUB_USER}/{REPO_NAME}/main/pronosticos/pronostico_{nombre_base}.json",
    }

# Estad√≠sticas generales
fechas_todas = []
for var in variables:
    fechas_todas.extend(df_hourly[var].index)

if fechas_todas:
    fechas_todas = pd.Series(fechas_todas)
    indice_pronosticos["estadisticas_generales"] = {
        "rango_completo": {
            "inicio": fechas_todas.min().strftime("%Y-%m-%d %H:%M:%S"),
            "fin": fechas_todas.max().strftime("%Y-%m-%d %H:%M:%S"),
            "total_dias": (fechas_todas.max() - fechas_todas.min()).days,
        },
        "total_registros_por_hora": len(df_hourly),
        "fecha_ultima_actualizacion": datetime.now().isoformat(),
    }

# Guardar y subir √≠ndice
with open("index.json", "w", encoding="utf-8") as f:
    json.dump(indice_pronosticos, f, ensure_ascii=False, indent=2)

print("  üìÑ Procesando: index.json")
if subir_a_github_json("index.json", indice_pronosticos, mensaje_commit):
    exitosos += 1
    print("    ‚úÖ √çndice subido exitosamente")

# 7. üéØ RESUMEN COMPLETO
print(f"\n{'='*80}")
print("üéØ RESUMEN FINAL DE DATOS GENERADOS")
print(f"{'='*80}")

for var in variables:
    serie = df_hourly[var]

    # Determinar nombre base consistente
    nombre_base = var.lower().replace(" ", "").replace(".", "").replace("pm", "pm")
    if nombre_base == "temperature":
        nombre_base = "temperature"
    elif nombre_base == "humidity":
        nombre_base = "humidity"
    elif nombre_base == "pm25":
        nombre_base = "pm2.5"
    elif nombre_base == "pm10":
        nombre_base = "pm10"

    print(f"\nüìä {var}:")
    print(f"   ‚Ä¢ Datos hist√≥ricos: {len(serie)} registros")
    print(f"   ‚Ä¢ Desde: {serie.index[0].strftime('%d/%m/%Y %H:%M')}")
    print(f"   ‚Ä¢ Hasta: {serie.index[-1].strftime('%d/%m/%Y %H:%M')}")
    print(f"   ‚Ä¢ Rango: {(serie.index[-1] - serie.index[0]).days} d√≠as")
    print(f"   ‚Ä¢ Archivo JSON: pronostico_{nombre_base}.json")
    print(f"   ‚Ä¢ Archivo CSV: datos_{nombre_base}.csv")

print(
    f"\nüìÅ Total de archivos generados: {len(variables) * 2 + 1}"
)  # JSON + CSV + √≠ndice
print(f"‚úÖ Subidos exitosamente: {exitosos}")

print(f"\nüåê Tus datos COMPLETOS est√°n en:")
print(f"   https://github.com/{GITHUB_USER}/{REPO_NAME}/tree/main/pronosticos")

print(f"\nüîó Endpoints principales:")
for var in variables:
    # Determinar nombre base consistente
    nombre_base = var.lower().replace(" ", "").replace(".", "").replace("pm", "pm")
    if nombre_base == "temperature":
        nombre_base = "temperature"
    elif nombre_base == "humidity":
        nombre_base = "humidity"
    elif nombre_base == "pm25":
        nombre_base = "pm2.5"
    elif nombre_base == "pm10":
        nombre_base = "pm10"

    print(
        f"   ‚Ä¢ {var}: https://raw.githubusercontent.com/{GITHUB_USER}/{REPO_NAME}/main/pronosticos/pronostico_{nombre_base}.json"
    )

print(f"\nüìä Para verificar los datos:")
print(f"   1. Ve a: https://github.com/{GITHUB_USER}/{REPO_NAME}/tree/main/pronosticos")
print(f"   2. Click en cualquier archivo .json")
print(f"   3. Click en 'Raw' para ver los datos completos")

print(f"\nüöÄ Tu p√°gina web se actualizar√° autom√°ticamente en:")
print(f"   https://{GITHUB_USER}.github.io/{REPO_NAME}/")

print("\n" + "=" * 80)
print(" ‚úÖ PROCESO COMPLETADO - TODOS LOS DATOS INCLUIDOS")
print("=" * 80)

# 8. VERIFICACI√ìN FINAL (MUESTRA EJEMPLO)
print(f"\nüîç Verificaci√≥n de datos incluidos:")

if len(variables) > 0:
    primera_var = variables[0]

    # Determinar nombre base consistente
    nombre_base = (
        primera_var.lower().replace(" ", "").replace(".", "").replace("pm", "pm")
    )
    if nombre_base == "temperature":
        nombre_base = "temperature"
    elif nombre_base == "humidity":
        nombre_base = "humidity"
    elif nombre_base == "pm25":
        nombre_base = "pm2.5"
    elif nombre_base == "pm10":
        nombre_base = "pm10"

    primer_json = f"pronostico_{nombre_base}.json"

    if os.path.exists(primer_json):
        with open(primer_json, "r", encoding="utf-8") as f:
            datos = json.load(f)

        print(f"\nüìã Ejemplo de datos en {primer_json}:")
        print(f"  Variable: {datos['variable']}")
        print(f"  Total hist√≥rico: {len(datos.get('historico', []))} registros")

        if datos.get("historico"):
            print(f"  Primer dato: {datos['historico'][0]['fecha']}")
            print(f"  √öltimo hist√≥rico: {datos['historico'][-1]['fecha']}")

        if datos.get("pronosticos"):
            print(f"  Primer pron√≥stico: {datos['pronosticos'][0]['fecha']}")
            print(f"  √öltimo pron√≥stico: {datos['pronosticos'][-1]['fecha']}")
            print(f"  Total pron√≥sticos: {datos['total_pronosticos']} horas")

        # Verificar estructura
        print(f"  ‚úÖ Estructura correcta: 'historico' en lugar de 'historial_completo'")

print("\n‚ö†Ô∏è  IMPORTANTE: Aseg√∫rate de que tu dashboard JavaScript use:")
print("   - 'historico' en lugar de 'historial_completo'")
print("   - Los mismos nombres de archivo: pronostico_temperature.json, etc.")