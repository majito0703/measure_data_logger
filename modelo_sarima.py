# -*- coding: utf-8 -*-
"""Modelo_Sarima.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iAFMXztN6AvkjVaC3_b2wtbi-jGTxS8R
"""

import os
import pickle
import json
import warnings
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from datetime import datetime, timedelta

warnings.filterwarnings("ignore")

# ======================================================
# URL DE GOOGLE SHEETS (definida a nivel global)
# ======================================================
SHEET_URL = "https://docs.google.com/spreadsheets/d/1x1FeUolFWlR07tgrc6F4cgeUhJYV7uQ5yuRTBHO8jWI/edit?gid=0#gid=0"

# ======================================================
# CONFIGURACI√ìN DE CARPETA DE SALIDA SIMPLIFICADA
# ======================================================
def configurar_carpeta_salida():
    """Configura la carpeta donde se guardar√°n los resultados JSON"""
    try:
        # En Colab, guardar en Drive
        if 'IN_COLAB' in globals() and IN_COLAB:
            carpeta_base = "/content/drive/MyDrive/pronosticos"
        else:
            # En GitHub Actions o local
            carpeta_base = "./pronosticos"
        
        # Crear solo una carpeta
        os.makedirs(carpeta_base, exist_ok=True)
        print(f"‚úÖ Carpeta creada/verificada: {carpeta_base}")
        
        return carpeta_base
    except Exception as e:
        print(f"‚ö†Ô∏è  Error configurando carpeta: {e}")
        return None

# ======================================================
# 1. FUNCI√ìN DE CONEXI√ìN A GOOGLE SHEETS (COLAB + GITHUB)
# ======================================================


def conectar_a_google_sheets():
    """
    Conecta a Google Sheets de manera inteligente
    - En Colab: usa autenticaci√≥n normal
    - En GitHub: puede usar Service Account
    """

    try:
        # Verificar si estamos en Google Colab
        try:
            from google.colab import auth

            IN_COLAB = True
        except:
            IN_COLAB = False

        if IN_COLAB:
            # ========== MODO COLAB ==========
            print("üîë Autenticando en Google Colab...")

            # Autenticaci√≥n interactiva
            auth.authenticate_user()
            from google.auth import default

            creds, _ = default()

            import gspread

            gc = gspread.authorize(creds)

        else:
            # ========== MODO GITHUB/LOCAL ==========
            import gspread

            # Intentar con variable de entorno (para GitHub Actions)
            creds_json = os.getenv("GOOGLE_SHEETS_CREDS")

            if creds_json:
                print("üîë Usando Service Account desde variable de entorno...")
                from google.oauth2.service_account import Credentials

                creds_dict = json.loads(creds_json)
                scope = ["https://www.googleapis.com/auth/spreadsheets"]
                credentials = Credentials.from_service_account_info(
                    creds_dict, scopes=scope
                )
                gc = gspread.authorize(credentials)
            else:
                # Intentar autenticaci√≥n normal (fallback)
                print("‚ö†Ô∏è  Intentando autenticaci√≥n normal...")
                gc = gspread.oauth()  # Esto abrir√° navegador en local

        # Abrir la hoja
        spreadsheet = gc.open_by_url(SHEET_URL)
        worksheet = spreadsheet.get_worksheet(0)

        print("‚úÖ Conexi√≥n exitosa a Google Sheets")
        return worksheet

    except Exception as e:
        print(f"‚ùå Error conectando a Google Sheets: {e}")
        print("‚ö†Ô∏è  Usando datos de ejemplo para continuar...")
        return None


# ======================================================
# 2. CONFIGURACI√ìN INICIAL DE COLAB
# ======================================================

# Montar Google Drive (solo funciona en Colab)
try:
    from google.colab import drive

    drive.mount("/content/drive", force_remount=False)
    IN_COLAB = True
    print("‚úÖ Google Drive montado")

except:
    IN_COLAB = False
    print("‚ö†Ô∏è  No es Google Colab, omitiendo montaje de Drive")

# ======================================================
# 3. INSTALAR DEPENDENCIAS (SOLO COLAB)
# ======================================================

if IN_COLAB:
    print("üì¶ Instalando dependencias en Colab...")
else:
    print("‚úÖ En GitHub, las dependencias se instalan desde requirements.txt")

# ======================================================
# 4. IMPORTAR BIBLIOTECAS RESTANTES
# ======================================================

import gspread
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
from statsmodels.tsa.statespace.sarimax import SARIMAX
import matplotlib.dates as mdates

# Cargar jupyter-black (solo Colab)
if IN_COLAB:
    import jupyter_black

    jupyter_black.load()

# ======================================================
# 5. REEMPLAZAR LA IMPORTACI√ìN DE 'direl_ts_tool_kit'
# ======================================================


def parse_datetime_index(df, date_column="date", format="%d/%m/%Y %H:%M:%S"):
    """
    Convierte una columna de fecha a datetime y la usa como √≠ndice
    (Reemplaza a la funci√≥n de direl_ts_tool_kit)
    """
    df_copy = df.copy()
    df_copy[date_column] = pd.to_datetime(
        df_copy[date_column], format=format, errors="coerce"
    )
    df_copy.set_index(date_column, inplace=True)
    return df_copy


def plot_time_series_with_forecast(df_historico, df_pronostico, variable, units="", time_unit="Day", pasos_pronostico=72):
    """
    Grafica una serie de tiempo con datos hist√≥ricos y pron√≥stico
    """
    fig, ax = plt.subplots(figsize=(14, 6))

    # Plot datos hist√≥ricos (√∫ltimas 1200 - 2 d√≠as)
    ax.plot(df_historico.index, df_historico[variable], 
            linewidth=1.5, color='blue', alpha=0.7, label='Datos hist√≥ricos')
    
    # Plot datos de pron√≥stico (72 horas)
    ax.plot(df_pronostico.index, df_pronostico['pronostico'], 
            linewidth=2, color='red', label='Pron√≥stico 72h')
    
    # Sombrear intervalo de confianza 80%
    ax.fill_between(df_pronostico.index, 
                    df_pronostico['limite_inferior_80'], 
                    df_pronostico['limite_superior_80'],
                    alpha=0.2, color='orange', label='Intervalo confianza 80%')
    
    # Sombrear intervalo de confianza 95%
    ax.fill_between(df_pronostico.index, 
                    df_pronostico['limite_inferior_95'], 
                    df_pronostico['limite_superior_95'],
                    alpha=0.1, color='gray', label='Intervalo confianza 95%')
    
    ax.set_title(f"Serie de Tiempo y Pron√≥stico - {variable}")
    ax.set_xlabel(f"Tiempo ({time_unit})")
    ax.set_ylabel(f"{variable} {units}")
    ax.grid(True, alpha=0.3)
    ax.legend(loc='best')

    # Formato de fechas
    ax.xaxis.set_major_formatter(mdates.DateFormatter("%d/%m %H:%M"))
    plt.setp(ax.xaxis.get_majorticklabels(), rotation=45)

    plt.tight_layout()
    return fig


# ======================================================
# 6. CONECTAR Y CARGAR DATOS
# ======================================================

print("\n" + "=" * 60)
print("üì• CARGANDO DATOS DESDE GOOGLE SHEETS")
print("=" * 60)

# Conectar a Google Sheets
worksheet = conectar_a_google_sheets()

if worksheet is not None:
    # Obtener todos los datos
    datos = worksheet.get_all_values()

    # Convertir a DataFrame
    df0 = pd.DataFrame(datos[1:], columns=datos[0])

    print(f"‚úÖ Datos cargados desde Google Sheets")
    print(f"üìä Dimensiones: {df0.shape[0]} filas √ó {df0.shape[1]} columnas")
    print(f"üîó URL: {SHEET_URL}")

else:
    # ========== DATOS DE EJEMPLO (si falla la conexi√≥n) ==========
    print("üìù Usando datos de ejemplo para demostraci√≥n...")

    # Crear datos de ejemplo simples
    import numpy as np

    fechas = pd.date_range(start="2024-01-01", periods=100, freq="H")

    datos_ejemplo = {
        "Date": [f.strftime("%d/%m/%Y %H:%M:%S") for f in fechas],
        "Temperature": 25 + 5 * np.sin(np.linspace(0, 10, 100)),
        "Humidity": 60 + 10 * np.cos(np.linspace(0, 8, 100)),
        "PM 2.5(¬µg/m¬≥)": 20 + 8 * np.random.randn(100),
        "PM 10 (¬µg/m¬≥)": 40 + 12 * np.random.randn(100),
        "PM 1.0 (¬µg/m¬≥)": 10 + 4 * np.random.randn(100),
    }

    df0 = pd.DataFrame(datos_ejemplo)
    print(f"üìä Datos de ejemplo creados: {df0.shape[0]} filas")

print("\nüìã Primeras filas de datos:")
print(df0.head(2))

# ======================================================
# 6.1. VERIFICACI√ìN DE ACTUALIZACI√ìN DE DATOS
# ======================================================
print("\nüìÖ Verificando actualizaci√≥n de datos...")
print(f"√öltima fecha en datos: {df0['Date'].iloc[-1]}")
print(f"Total de registros: {len(df0)}")

# Verificar si hay datos recientes
try:
    ultima_fecha_str = df0['Date'].iloc[-1]
    ultima_fecha = pd.to_datetime(ultima_fecha_str, format="%d/%m/%Y %H:%M:%S", errors='coerce')
    if ultima_fecha:
        hoy = datetime.now()
        diferencia = hoy - ultima_fecha
        print(f"D√≠as desde el √∫ltimo registro: {diferencia.days}")
        
        if diferencia.days > 7:
            print("‚ö†Ô∏è  ADVERTENCIA: Los datos pueden estar desactualizados")
        else:
            print("‚úÖ Datos actualizados recientemente")
except Exception as e:
    print(f"‚ö†Ô∏è  Error verificando actualizaci√≥n: {e}")

df0.rename(
    columns={
        "Date": "date",
        "PM 1.0 (¬µg/m¬≥)": "PM 1",
        "PM 2.5(¬µg/m¬≥)": "PM 2.5",
        "PM 10 (¬µg/m¬≥)": "PM 10",
    },
    inplace=True,
)

# ======================================================
# CONVERSI√ìN DE DATOS Y LIMPIEZA
# ======================================================

print("üîç Verificando tipos de datos iniciales:")
print(df0.dtypes)

# Identificar columnas que deber√≠an ser num√©ricas
columnas_numericas = ["Temperature", "Humidity", "PM 2.5", "PM 10", "PM 1"]

# Convertir cada columna a num√©rico, forzando errores a NaN
for col in columnas_numericas:
    if col in df0.columns:
        print(f"\nConvirtiendo columna '{col}'...")
        
        # Guardar valores originales para comparar
        valores_originales = df0[col].head(5).tolist()
        
        # Convertir a num√©rico
        df0[col] = pd.to_numeric(df0[col], errors='coerce')
        
        # Contar valores convertidos y no convertidos
        total_valores = len(df0[col])
        valores_nan = df0[col].isna().sum()
        valores_convertidos = total_valores - valores_nan
        
        print(f"  Valores originales (primeros 5): {valores_originales}")
        print(f"  Valores convertidos: {valores_convertidos}/{total_valores}")
        print(f"  Valores no convertidos (NaN): {valores_nan}")

# Crear DataFrame con √≠ndice temporal
df1 = parse_datetime_index(df0, format="%d/%m/%Y %H:%M:%S")

print("\n‚úÖ Tipos de datos despu√©s de la conversi√≥n:")
print(df1.dtypes)

# ======================================================
# LIMPIEZA ADICIONAL ANTES DEL RESAMPLE (SOLUCI√ìN AL ERROR)
# ======================================================

print("\nüßπ Realizando limpieza adicional antes del resample...")

# 1. Eliminar columnas vac√≠as o con nombres vac√≠os
df1 = df1.loc[:, ~df1.columns.astype(str).str.contains('^Unnamed')]
df1 = df1.loc[:, ~df1.columns.astype(str).str.contains('^$')]

# 2. Seleccionar solo columnas num√©ricas conocidas
columnas_a_mantener = ['Temperature', 'Humidity', 'PM 1', 'PM 2.5', 'PM 10']
columnas_existentes = [col for col in columnas_a_mantener if col in df1.columns]

print(f"üìã Columnas a mantener: {columnas_existentes}")

# 3. Verificar que todas las columnas sean num√©ricas
for col in columnas_existentes:
    if df1[col].dtype not in ['int64', 'float64']:
        print(f"‚ö†Ô∏è  Columna '{col}' no es num√©rica, convirtiendo...")
        df1[col] = pd.to_numeric(df1[col], errors='coerce')

# 4. Eliminar filas donde todas las columnas sean NaN
df1 = df1.dropna(how='all')

print(f"\nüìä DataFrame despu√©s de limpieza:")
print(f"  - Forma: {df1.shape[0]} filas √ó {df1.shape[1]} columnas")
print(f"  - Tipos de datos: {df1.dtypes.to_dict()}")

# ======================================================
# RESAMPLE CON SEGURIDAD
# ======================================================

print("\nüìä Realizando resample a 10 minutos...")
try:
    # Opci√≥n 1: Usar numeric_only=True para seguridad
    df1 = df1.resample("10min").mean(numeric_only=True)
    print("‚úÖ Resample completado exitosamente con numeric_only=True")
    
except Exception as e:
    print(f"‚ö†Ô∏è  Error con numeric_only=True: {e}")
    print("üîÑ Intentando m√©todo alternativo...")
    
    # Opci√≥n 2: M√©todo alternativo manual
    try:
        # Crear lista para almacenar resultados
        resample_data = []
        
        # Para cada columna num√©rica
        for col in df1.columns:
            if df1[col].dtype in ['int64', 'float64']:
                # Resample por columna
                col_resampled = df1[col].resample("10min").mean()
                resample_data.append(col_resampled)
        
        # Combinar resultados
        df1 = pd.concat(resample_data, axis=1)
        print("‚úÖ Resample completado exitosamente con m√©todo alternativo")
        
    except Exception as e2:
        print(f"‚ùå Error en m√©todo alternativo: {e2}")
        print("‚ö†Ô∏è  Continuando sin resample...")
        # Mantener df1 sin cambios

print(f"üìä Nueva forma despu√©s del resample: {df1.shape[0]} filas √ó {df1.shape[1]} columnas")

# Mostrar estad√≠sticas
print("\nüìà Estad√≠sticas descriptivas:")
print(df1.describe().transpose())

# ======================================================
# FUNCIONES ORIGINALES DE GRAFICADO (sin pron√≥stico a√∫n)
# ======================================================
def plot_time_series(df, variable, units="", time_unit="Day"):
    """
    Grafica una serie de tiempo
    (Versi√≥n simplificada de la funci√≥n original)
    """
    fig, ax = plt.subplots(figsize=(12, 5))

    ax.plot(df.index, df[variable], linewidth=1)
    ax.set_title(f"Serie de Tiempo - {variable}")
    ax.set_xlabel(f"Tiempo ({time_unit})")
    ax.set_ylabel(f"{variable} {units}")
    ax.grid(True, alpha=0.3)

    # Formato de fechas
    ax.xaxis.set_major_formatter(mdates.DateFormatter("%d/%m %H:%M"))
    plt.setp(ax.xaxis.get_majorticklabels(), rotation=45)

    plt.tight_layout()
    return fig

# Graficar series originales
fig = plot_time_series(df1, variable="Temperature", units="¬∞C", time_unit="Day")
plt.show()

fig = plot_time_series(df1, variable="Humidity", units="%", time_unit="Day")
plt.show()

fig = plot_time_series(df1, variable="PM 10", units="¬µg/m¬≥", time_unit="Day")
plt.show()

fig = plot_time_series(df1, variable="PM 2.5", units="¬µg/m¬≥", time_unit="Day")
plt.show()

df2 = df1.copy()

vars_to_impute = ["Temperature", "Humidity", "PM 10", "PM 2.5"]

for var in vars_to_impute:
    # Marcar NaN antes de imputar
    df2[f"{var}_imputed"] = df2[var].isna()

    # Calcular medias por hora:minuto
    means = df2.groupby(df2.index.time)[var].transform("mean")

    # Llenar SOLO los NaN, sin alterar valores originales
    df2[var] = df2[var].fillna(means)

df2 = df2.loc[:, ~df2.columns.str.endswith("_imputed")]
# Ver primeras filas del nuevo DataFrame
print(df2.head())

# Ver solo los datos imputados de cada variable
print(df2[[col for col in df2.columns if "imputed" in col]].sum())
df2 = df2.loc[:, ~df2.columns.str.endswith("_imputed")]

fig = plot_time_series(df2, variable="Temperature", units="¬∞C", time_unit="Day")
plt.show()

fig = plot_time_series(df2, variable="Humidity", units="%", time_unit="Day")
plt.show()

fig = plot_time_series(df2, variable="PM 10", units="¬µg/m¬≥", time_unit="Day")
plt.show()

fig = plot_time_series(df2, variable="PM 2.5", units="¬µg/m¬≥", time_unit="Day")
plt.show()

# ======================================================
# MODIFICACI√ìN CR√çTICA: SELECCI√ìN DE √öLTIMOS 1200 DATOS Y ELIMINACI√ìN DE √öLTIMOS 2 D√çAS
# ======================================================
print("\n" + "="*60)
print("üìä SELECCI√ìN DE DATOS PARA MODELADO")
print("="*60)

# Copiar el DataFrame
df3 = df2.copy()

print(f"üìä Datos totales disponibles: {len(df3)} registros")

# ======================================================
# PASO 1: SELECCIONAR √öLTIMOS 1200 DATOS
# ======================================================

# Obtener los √∫ltimos 1200 datos (aproximadamente 200 horas con datos cada 10 minutos)
# Si hay menos de 1200 datos, usar todos los disponibles
total_datos = len(df3)
datos_a_usar = min(1200, total_datos)

# Seleccionar los √∫ltimos N datos
df3_temp = df3.tail(datos_a_usar).copy()

print(f"‚úÖ √öltimos {len(df3_temp)} datos seleccionados para procesar")
print(f"üìÖ Rango inicial: {df3_temp.index[0]} a {df3_temp.index[-1]}")

# ======================================================
# PASO 2: ELIMINAR LOS √öLTIMOS 2 D√çAS DE ESOS 1200 DATOS
# ======================================================

# Encontrar las fechas √∫nicas en los datos seleccionados
fechas_unicas = sorted(set(df3_temp.index.date))
print(f"\nüìÖ Fechas √∫nicas en los datos seleccionados: {len(fechas_unicas)} d√≠as")

if len(fechas_unicas) > 2:
    # Seleccionar los DOS √∫ltimos d√≠as
    dias_a_eliminar = fechas_unicas[-2:]  # Los dos √∫ltimos d√≠as
    
    print(f"üóëÔ∏è  Eliminando datos de los √∫ltimos 2 d√≠as:")
    for dia in dias_a_eliminar:
        print(f"   - {dia.strftime('%Y-%m-%d')}")
    
    # Convertir a lista de strings para comparaci√≥n
    dias_a_eliminar_str = [d.strftime("%Y-%m-%d") for d in dias_a_eliminar]
    
    # Crear una m√°scara booleana
    mask = df3_temp.index.strftime("%Y-%m-%d").isin(dias_a_eliminar_str)
    
    # Contar cu√°ntos registros se eliminar√°n
    registros_a_eliminar = mask.sum()
    print(f"   Total de registros a eliminar: {registros_a_eliminar}")
    
    # Eliminar los dos √∫ltimos d√≠as
    df3 = df3_temp[~mask].copy()
    
    print(f"\n‚úÖ Datos despu√©s de eliminar √∫ltimos 2 d√≠as:")
    print(f"   - Registros restantes: {len(df3)}")
    print(f"   - Rango final: {df3.index[0]} a {df3.index[-1]}")
    
else:
    # Si hay 2 d√≠as o menos, usar todos los datos disponibles
    print(f"‚ö†Ô∏è  Solo hay {len(fechas_unicas)} d√≠as de datos, usando todos disponibles")
    df3 = df3_temp.copy()

# ======================================================
# PASO 3: ELIMINAR COLUMNA PM 1 (si existe)
# ======================================================
df3 = df3.drop(columns=["PM 1"], errors="ignore")

# Verificaci√≥n final
print(f"\nüìã Resumen final:")
print(f"   - Total registros para modelado: {len(df3)}")
print(f"   - Columnas: {list(df3.columns)}")
print(f"   - Rango temporal: {df3.index[0]} a {df3.index[-1]}")
print(f"   - D√≠as cubiertos: {(df3.index[-1] - df3.index[0]).days} d√≠as")

# Mostrar las primeras y √∫ltimas filas
print(f"\nüîç Primeras 3 filas:")
print(df3.head(3))
print(f"\nüîç √öltimas 3 filas:")
print(df3.tail(3))

# ======================================================
# GUARDAR DF3 PARA USO POSTERIOR EN GR√ÅFICAS
# ======================================================
# Este df3 contiene los datos hist√≥ricos sin los √∫ltimos 2 d√≠as
df_historico = df3.copy()

# ======================================================
# CONFIGURAR CARPETA DE SALIDA SIMPLIFICADA
# ======================================================
print("\n" + "=" * 60)
print("üìÅ CONFIGURANDO CARPETA DE SALIDA (pronosticos/)")
print("=" * 60)

carpeta_pronosticos = configurar_carpeta_salida()

# ======================================================
# 1. RESAMPLEO A DATOS POR HORA CON FRECUENCIA EXPL√çCITA
# ======================================================
# Asegurar que el √≠ndice tenga frecuencia para SARIMA
df_hourly = df3.resample("H").mean().dropna()

# Asignar frecuencia expl√≠cita al √≠ndice (IMPORTANTE PARA SARIMA)
if not df_hourly.index.freq:
    print("‚ö†Ô∏è  Asignando frecuencia horaria al √≠ndice...")
    df_hourly = df_hourly.asfreq('H')

variables = ["Temperature", "Humidity", "PM 2.5", "PM 10"]

# ======================================================
# 2. OPTIMIZADOR SARIMA (SIN PMDARIMA)
# ======================================================
p = d = q = [0, 1]
P = D = Q = [0, 1]
m = 12  # estacionalidad de 12 horas


def buscar_mejor_modelo(series):
    mejor_aic = float("inf")
    mejor_modelo = None
    mejor_orden = None
    mejor_orden_season = None
    mejores_parametros = None
    mejor_summary = None

    for pi in p:
        for di in d:
            for qi in q:
                for Pi in P:
                    for Di in D:
                        for Qi in Q:
                            orden = (pi, di, qi)
                            orden_seas = (Pi, Di, Qi, m)

                            try:
                                modelo = SARIMAX(
                                    series,
                                    order=orden,
                                    seasonal_order=orden_seas,
                                    enforce_stationarity=False,
                                    enforce_invertibility=False,
                                ).fit(disp=False, maxiter=200)

                                if modelo.aic < mejor_aic:
                                    mejor_aic = modelo.aic
                                    mejor_modelo = modelo
                                    mejor_orden = orden
                                    mejor_orden_season = orden_seas
                                    mejores_parametros = modelo.params
                                    mejor_summary = modelo.summary()

                            except:
                                continue

    return (
        mejor_modelo,
        mejor_orden,
        mejor_orden_season,
        mejor_aic,
        mejores_parametros,
        mejor_summary,
    )

# ======================================================
# 3. FUNCI√ìN PARA OBTENER ECUACI√ìN MATEM√ÅTICA
# ======================================================
def obtener_ecuacion_sarima(modelo, orden, orden_seas):
    """
    Genera la ecuaci√≥n matem√°tica del modelo SARIMA
    """
    p, d, q = orden
    P, D, Q, m = orden_seas

    # Obtener par√°metros
    params = modelo.params

    # Inicializar partes de la ecuaci√≥n
    parte_ar = ""
    parte_ma = ""
    parte_sar = ""
    parte_sma = ""

    # Coeficientes AR (no estacional)
    for i in range(1, p + 1):
        if f"ar.L{i}" in params:
            coef = params[f"ar.L{i}"]
            parte_ar += f" + {coef:.4f}¬∑y_t-{i}"

    # Coeficientes MA (no estacional)
    for i in range(1, q + 1):
        if f"ma.L{i}" in params:
            coef = params[f"ma.L{i}"]
            parte_ma += f" + {coef:.4f}¬∑Œµ_t-{i}"

    # Coeficientes SAR (estacional)
    for i in range(1, P + 1):
        if f"ar.S.L{m*i}" in params:
            coef = params[f"ar.S.L{m*i}"]
            parte_sar += f" + {coef:.4f}¬∑y_t-{m*i}"

    # Coeficientes SMA (estacional)
    for i in range(1, Q + 1):
        if f"ma.S.L{m*i}" in params:
            coef = params[f"ma.S.L{m*i}"]
            parte_sma += f" + {coef:.4f}¬∑Œµ_t-{m*i}"

    # Constante
    constante = ""
    if "intercept" in params:
        constante = f"{params['intercept']:.4f} + "
    elif "const" in params:
        constante = f"{params['const']:.4f} + "

    # Construir ecuaci√≥n
    if d == 0 and D == 0:
        ecuacion = f"y_t = {constante}"
    else:
        # Para modelos con diferenciaci√≥n
        ecuacion = "Œî^d Œî_s^D y_t = "
        if constante.strip():
            ecuacion = f"Œî^d Œî_s^D y_t = {constante}"

    # Agregar partes
    if parte_ar:
        ecuacion += parte_ar[3:] if ecuacion.endswith("= ") else parte_ar
    if parte_ma:
        ecuacion += parte_ma
    if parte_sar:
        ecuacion += parte_sar
    if parte_sma:
        ecuacion += parte_sma

    if parte_ar or parte_ma or parte_sar or parte_sma:
        ecuacion += " + Œµ_t"
    else:
        ecuacion += "Œµ_t"

    return ecuacion

# ======================================================
# 4. FUNCI√ìN PARA MOSTRAR PAR√ÅMETROS COMO EN LA IMAGEN
# ======================================================
def mostrar_parametros_tabla(modelo, orden, orden_seas, aic):
    """
    Muestra los par√°metros en formato de tabla como en la imagen
    """
    params = modelo.params
    p, d, q = orden
    P, D, Q, m = orden_seas

    print("\n" + "=" * 60)
    print("PAR√ÅMETROS DEL MODELO SARIMA")
    print("=" * 60)

    # Crear lista de par√°metros
    parametros_lista = []

    # Par√°metros AR
    for i in range(1, p + 1):
        key = f"ar.L{i}"
        if key in params:
            parametros_lista.append((f"œÜ_{i}", params[key], modelo.bse.get(key, "N/A")))

    # Par√°metros MA
    for i in range(1, q + 1):
        key = f"ma.L{i}"
        if key in params:
            parametros_lista.append((f"Œ∏_{i}", params[key], modelo.bse.get(key, "N/A")))

    # Par√°metros SAR
    for i in range(1, P + 1):
        key = f"ar.S.L{m*i}"
        if key in params:
            parametros_lista.append((f"Œ¶_{i}", params[key], modelo.bse.get(key, "N/A")))

    # Par√°metros SMA
    for i in range(1, Q + 1):
        key = f"ma.S.L{m*i}"
        if key in params:
            parametros_lista.append((f"Œò_{i}", params[key], modelo.bse.get(key, "N/A")))

    # Constante
    if "intercept" in params:
        parametros_lista.append(
            ("intercept", params["intercept"], modelo.bse.get("intercept", "N/A"))
        )
    elif "const" in params:
        parametros_lista.append(
            ("const", params["const"], modelo.bse.get("const", "N/A"))
        )

    # Varianza del error
    if "sigma2" in params:
        parametros_lista.append(
            ("œÉ¬≤", params["sigma2"], modelo.bse.get("sigma2", "N/A"))
        )

    # Mostrar tabla
    print(f"\nOrden: SARIMA{orden}{orden_seas}")
    print(f"AIC: {aic:.2f}")
    print("\n" + "-" * 60)
    print(f"{'Par√°metro':<15} {'Valor':<15} {'Error est√°ndar':<15}")
    print("-" * 60)

    for nombre, valor, error in parametros_lista:
        if isinstance(error, (int, float)):
            print(f"{nombre:<15} {valor:<15.4f} {error:<15.4f}")
        else:
            print(f"{nombre:<15} {valor:<15.4f} {str(error):<15}")

    print("-" * 60)

# ======================================================
# 5. FUNCI√ìN CORREGIDA PARA GUARDAR PRON√ìSTICO COMO JSON
# (AHORA GUARDA TODAS LAS 72 HORAS)
# ======================================================
def guardar_pronostico_json(modelo, serie, variable, df_historico, pasos=72):
    """Guarda el pron√≥stico completo (72 horas) en un archivo JSON"""
    try:
        if carpeta_pronosticos is None:
            print("‚ö†Ô∏è  No se pudo crear carpeta, no se guardar√° el pron√≥stico")
            return None
        
        # Obtener pron√≥stico de 72 horas
        pred = modelo.get_forecast(steps=pasos)
        media = pred.predicted_mean
        conf_80 = pred.conf_int(alpha=0.20)
        conf_95 = pred.conf_int(alpha=0.05)
        
        # Obtener √∫ltima fecha hist√≥rica
        ultima_fecha_historica = serie.index[-1]
        
        # Inferir frecuencia (debe ser 'H' para horas)
        freq = pd.infer_freq(serie.index) or 'H'
        
        # Crear fechas para el pron√≥stico (72 horas)
        fechas_pronostico = pd.date_range(
            start=ultima_fecha_historica + pd.Timedelta(hours=1), 
            periods=pasos, 
            freq=freq
        )
        
        # Preparar datos del pron√≥stico (TODAS LAS 72 HORAS)
        pronostico_data = []
        for i in range(pasos):
            pronostico_data.append({
                'fecha': fechas_pronostico[i].strftime('%Y-%m-%d %H:%M:%S'),
                'pronostico': float(media.values[i]),
                'limite_inferior_80': float(conf_80.iloc[i, 0]),
                'limite_superior_80': float(conf_80.iloc[i, 1]),
                'limite_inferior_95': float(conf_95.iloc[i, 0]),
                'limite_superior_95': float(conf_95.iloc[i, 1])
            })
        
        # Preparar datos hist√≥ricos recientes (√∫ltimas 24 horas para contexto)
        ultimas_24h = 24
        datos_historicos = []
        
        # Verificar que haya suficientes datos hist√≥ricos
        if len(serie) >= ultimas_24h:
            datos_recientes = serie.tail(ultimas_24h)
        else:
            datos_recientes = serie.tail(len(serie))
        
        for i, (fecha, valor) in enumerate(zip(datos_recientes.index, datos_recientes.values)):
            datos_historicos.append({
                'fecha': fecha.strftime('%Y-%m-%d %H:%M:%S'),
                'valor_real': float(valor)
            })
        
        # Informaci√≥n del modelo
        orden = modelo.specification.order
        orden_seas = modelo.specification.seasonal_order
        
        # Obtener ecuaci√≥n
        ecuacion = obtener_ecuacion_sarima(modelo, orden, orden_seas)
        
        # Crear estructura completa del JSON
        datos_completos = {
            'metadata': {
                'variable': variable,
                'modelo': f'SARIMA{orden}{orden_seas}',
                'fecha_entrenamiento': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                'fecha_ultimo_dato_historico': ultima_fecha_historica.strftime('%Y-%m-%d %H:%M:%S'),
                'valor_ultimo_dato': float(serie.iloc[-1]),
                'horizonte_pronostico': pasos,
                'unidades': get_units(variable),
                'ecuacion': ecuacion,
                'metricas': {
                    'aic': float(modelo.aic),
                    'bic': float(modelo.bic),
                    'hqic': float(modelo.hqic),
                    'log_likelihood': float(modelo.llf),
                    'num_observaciones': int(modelo.nobs)
                }
            },
            'parametros': {str(k): float(v) for k, v in modelo.params.items()},
            'datos_historicos_recientes': datos_historicos,  # √öltimas 24 horas
            'pronostico': pronostico_data  # TODAS LAS 72 HORAS
        }
        
        # Guardar en JSON
        fecha_actual = datetime.now().strftime('%Y%m%d_%H%M')
        nombre_archivo = f"pronostico_{variable}.json"
        ruta_completa = os.path.join(carpeta_pronosticos, nombre_archivo)
        
        with open(ruta_completa, 'w', encoding='utf-8') as f:
            json.dump(datos_completos, f, indent=2, ensure_ascii=False)
        
        print(f"‚úÖ Pron√≥stico JSON guardado en: {ruta_completa}")
        print(f"   - 72 horas de pron√≥stico guardadas")
        print(f"   - {len(datos_historicos)} horas hist√≥ricas incluidas")
        
        return ruta_completa, pronostico_data
        
    except Exception as e:
        print(f"‚ö†Ô∏è  Error guardando pron√≥stico JSON para {variable}: {e}")
        import traceback
        traceback.print_exc()
        return None, None

def get_units(variable):
    """Obtiene las unidades para cada variable"""
    units = {
        "Temperature": "¬∞C",
        "Humidity": "%",
        "PM 2.5": "¬µg/m¬≥",
        "PM 10": "¬µg/m¬≥"
    }
    return units.get(variable, "")

# ======================================================
# 6. FUNCI√ìN PARA CREAR RESUMEN GENERAL COMO JSON
# ======================================================
def crear_resumen_general_json(resultados, ecuaciones, df_hourly, variables):
    """Crea un resumen general de todos los modelos en un solo JSON"""
    try:
        if carpeta_pronosticos is None:
            return None
            
        fecha_actual = datetime.now().strftime('%Y%m%d_%H%M')
        
        # Crear estructura del resumen
        resumen_general = {
            'metadata': {
                'fecha_generacion': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                'fecha_ultimo_dato': df_hourly.index[-1].strftime('%Y-%m-%d %H:%M:%S'),
                'num_variables': len(variables),
                'total_observaciones': len(df_hourly),
                'rango_datos': {
                    'inicio': df_hourly.index[0].strftime('%Y-%m-%d %H:%M:%S'),
                    'fin': df_hourly.index[-1].strftime('%Y-%m-%d %H:%M:%S')
                }
            },
            'modelos': {}
        }
        
        for var in variables:
            if var in resultados:
                modelo = resultados[var]
                orden = modelo.specification.order
                orden_seas = modelo.specification.seasonal_order
                
                resumen_general['modelos'][var] = {
                    'orden': str(orden),
                    'orden_estacional': str(orden_seas),
                    'ecuacion': ecuaciones.get(var, 'No disponible'),
                    'metricas': {
                        'aic': float(modelo.aic),
                        'bic': float(modelo.bic),
                        'hqic': float(modelo.hqic),
                        'log_likelihood': float(modelo.llf),
                        'num_observaciones': int(modelo.nobs)
                    },
                    'ultimo_valor': float(df_hourly[var].iloc[-1]),
                    'unidades': get_units(var)
                }
        
        # Guardar resumen
        nombre_archivo = "resumen_modelos.json"
        ruta_completa = os.path.join(carpeta_pronosticos, nombre_archivo)
        
        with open(ruta_completa, 'w', encoding='utf-8') as f:
            json.dump(resumen_general, f, indent=2, ensure_ascii=False)
        
        print(f"‚úÖ Resumen general JSON guardado en: {ruta_completa}")
        
        return ruta_completa
        
    except Exception as e:
        print(f"‚ö†Ô∏è  Error creando resumen general JSON: {e}")
        return None

# ======================================================
# 7. EJECUCI√ìN COMPLETA POR VARIABLE CON MANEJO DE ERRORES
# ======================================================
limites = {
    "Temperature": None,
    "Humidity": None,
    "PM 2.5": 37,  # valor OMS o Colombia
    "PM 10": 75,
}

resultados = {}
ecuaciones = {}
archivos_guardados = {}
pronosticos_data = {}  # Para almacenar datos de pron√≥stico para graficar

print(f"\n{'='*80}")
print("üöÄ INICIANDO PROCESO DE MODELADO SARIMA")
print(f"{'='*80}")

for var in variables:
    print(f"\n{'='*80}")
    print(f" OPTIMIZANDO: {var}")
    print(f"{'='*80}")

    serie = df_hourly[var]

    try:
        modelo, orden, orden_s, aic, parametros, summary = buscar_mejor_modelo(serie)

        print(f"\nMejor modelo para {var}: SARIMA{orden}{orden_s}")
        print(f"AIC = {aic:.2f}")

        # Obtener y mostrar ecuaci√≥n matem√°tica
        try:
            ecuacion = obtener_ecuacion_sarima(modelo, orden, orden_s)
            print(f"\nEcuaci√≥n matem√°tica:")
            print(f"{ecuacion}")
        except Exception as e:
            print(f"‚ö†Ô∏è  Error obteniendo ecuaci√≥n: {e}")
            ecuacion = "No disponible"

        # Mostrar par√°metros en formato de tabla
        try:
            mostrar_parametros_tabla(modelo, orden, orden_s, aic)
        except Exception as e:
            print(f"‚ö†Ô∏è  Error mostrando par√°metros: {e}")

        # Guardar resultados en diccionarios
        resultados[var] = modelo
        ecuaciones[var] = ecuacion
        
        # Guardar pron√≥stico como JSON (CORREGIDO: ahora guarda todas las 72 horas)
        try:
            ruta_json, datos_pronostico = guardar_pronostico_json(modelo, serie, var, df_historico, pasos=72)
            if ruta_json:
                archivos_guardados[var] = ruta_json
                pronosticos_data[var] = datos_pronostico
        except Exception as e:
            print(f"‚ö†Ô∏è  Error guardando JSON para {var}: {e}")
            pronosticos_data[var] = None
            
    except Exception as e:
        print(f"‚ùå Error procesando {var}: {e}")
        print(f"‚ö†Ô∏è  Saltando variable {var}...")

# ======================================================
# 8. CREAR RESUMEN GENERAL COMO JSON
# ======================================================
try:
    ruta_resumen = crear_resumen_general_json(resultados, ecuaciones, df_hourly, variables)
    if ruta_resumen:
        archivos_guardados["resumen"] = ruta_resumen
except Exception as e:
    print(f"‚ö†Ô∏è  Error creando resumen general JSON: {e}")

# ======================================================
# 9. GRAFICAR DATOS HIST√ìRICOS + PRON√ìSTICO 72 HORAS
# ======================================================
print(f"\n{'='*80}")
print("üìà GENERANDO GR√ÅFICAS CON HIST√ìRICO Y PRON√ìSTICO")
print(f"{'='*80}")

for var in variables:
    if var in resultados and var in pronosticos_data and pronosticos_data[var] is not None:
        try:
            # Preparar DataFrame de pron√≥stico para graficar
            df_pronostico = pd.DataFrame(pronosticos_data[var])
            
            # Convertir fecha a datetime
            df_pronostico['fecha'] = pd.to_datetime(df_pronostico['fecha'])
            df_pronostico.set_index('fecha', inplace=True)
            
            # Obtener datos hist√≥ricos para esta variable
            # Usar df_historico (sin los √∫ltimos 2 d√≠as) en resoluci√≥n horaria para consistencia
            df_historico_hourly = df_historico.resample("H").mean()[var].dropna()
            
            # Graficar
            fig = plot_time_series_with_forecast(
                df_historico=df_historico_hourly,
                df_pronostico=df_pronostico,
                variable=var,
                units=get_units(var),
                time_unit="Day",
                pasos_pronostico=72
            )
            
            plt.show()
            
            # Tambi√©n guardar la gr√°fica
            if carpeta_pronosticos:
                nombre_grafica = f"grafica_{var}.png"
                ruta_grafica = os.path.join(carpeta_pronosticos, nombre_grafica)
                fig.savefig(ruta_grafica, dpi=150, bbox_inches='tight')
                print(f"‚úÖ Gr√°fica guardada: {ruta_grafica}")
                
        except Exception as e:
            print(f"‚ö†Ô∏è  Error generando gr√°fica para {var}: {e}")
    else:
        print(f"‚ö†Ô∏è  No se pudo generar gr√°fica para {var}: faltan datos")

# ======================================================
# 10. RESUMEN FINAL DE TODOS LOS MODELOS
# ======================================================
print(f"\n{'='*80}")
print(" üìä RESUMEN FINAL DE MODELOS SARIMA")
print(f"{'='*80}")

for var in variables:
    if var in resultados:
        print(f"\n{var}:")
        print(f"  Modelo: SARIMA{resultados[var].specification.order}")
        print(f"          {resultados[var].specification.seasonal_order}")
        print(f"  AIC: {resultados[var].aic:.2f}")
        print(f"  Ecuaci√≥n: {ecuaciones.get(var, 'No disponible')}")
        print(f"  N√∫mero de observaciones: {len(df_hourly[var])}")
        print(f"  √öltimo dato: {df_hourly[var].iloc[-1]:.2f} ({df_hourly.index[-1].strftime('%Y-%m-%d %H:%M')})")
        print(f"  Horas pronosticadas: 72")
    else:
        print(f"\n{var}: No se pudo ajustar modelo")

# ======================================================
# 11. RESUMEN DE ARCHIVOS GUARDADOS
# ======================================================
print(f"\n{'='*80}")
print(" üíæ ARCHIVOS GUARDADOS EN pronosticos/")
print(f"{'='*80}")

if archivos_guardados:
    for variable, ruta in archivos_guardados.items():
        if variable != "resumen":
            print(f"  JSON - {variable}: {os.path.basename(ruta)}")
    
    if "resumen" in archivos_guardados:
        print(f"  Resumen General: {os.path.basename(archivos_guardados['resumen'])}")
    
    print(f"\nüìä Total de archivos JSON generados: {len(archivos_guardados)}")
    
    # Mostrar contenido real de la carpeta
    if carpeta_pronosticos and os.path.exists(carpeta_pronosticos):
        archivos_en_carpeta = os.listdir(carpeta_pronosticos)
        archivos_json = [f for f in archivos_en_carpeta if f.endswith('.json')]
        archivos_png = [f for f in archivos_en_carpeta if f.endswith('.png')]
        
        print(f"\nüìÇ Contenido actual de {carpeta_pronosticos}:")
        
        print("  Archivos JSON:")
        for archivo in archivos_json:
            ruta_completa = os.path.join(carpeta_pronosticos, archivo)
            tama√±o = os.path.getsize(ruta_completa)
            tama√±o_kb = tama√±o / 1024
            
            # Verificar que contenga 72 horas de pron√≥stico
            with open(ruta_completa, 'r', encoding='utf-8') as f:
                datos = json.load(f)
                horas_pronostico = len(datos.get('pronostico', []))
            
            print(f"    üìÑ {archivo} ({tama√±o_kb:.1f} KB) - {horas_pronostico} horas pron√≥stico")
        
        if archivos_png:
            print("\n  Archivos PNG (gr√°ficas):")
            for archivo in archivos_png:
                ruta_completa = os.path.join(carpeta_pronosticos, archivo)
                tama√±o = os.path.getsize(ruta_completa)
                tama√±o_kb = tama√±o / 1024
                print(f"    üñºÔ∏è  {archivo} ({tama√±o_kb:.1f} KB)")
else:
    print("  No se guardaron archivos.")

print(f"\n{'='*80}")
print(" ‚úÖ PROCESO COMPLETADO EXITOSAMENTE")
print(f"{'='*80}")

# Mostrar ubicaci√≥n de la carpeta
if carpeta_pronosticos:
    print(f"\nüìÅ Carpeta de pron√≥sticos: {carpeta_pronosticos}")
    
    # Verificar contenido
    if os.path.exists(carpeta_pronosticos):
        archivos = os.listdir(carpeta_pronosticos)
        if archivos:
            print(f"üìã Total archivos en carpeta: {len(archivos)}")
        else:
            print("  üì≠ La carpeta est√° vac√≠a")
else:
    print("‚ö†Ô∏è  No se pudo crear la carpeta 'pronosticos'")

# ======================================================
# 12. RESUMEN DE ACTUALIZACI√ìN DE DATOS
# ======================================================
print(f"\n{'='*80}")
print(" üîÑ RESUMEN DE ACTUALIZACI√ìN DE DATOS")
print(f"{'='*80}")

if 'df0' in locals() and not df0.empty:
    print(f"üìä Datos cargados de Google Sheets:")
    print(f"  - Total registros: {len(df0)}")
    print(f"  - Primera fecha: {df0['date'].iloc[0]}")
    print(f"  - √öltima fecha: {df0['date'].iloc[-1]}")
    
    try:
        ultima_fecha_str = df0['date'].iloc[-1]
        ultima_fecha = pd.to_datetime(ultima_fecha_str, format="%d/%m/%Y %H:%M:%S")
        hoy = datetime.now()
        
        print(f"\nüìÖ An√°lisis de actualizaci√≥n:")
        print(f"  - √öltimo registro: {ultima_fecha.strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"  - Fecha actual: {hoy.strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"  - Diferencia: {(hoy - ultima_fecha).days} d√≠as")
        
        if (hoy - ultima_fecha).days <= 2:
            print("  ‚úÖ Datos actualizados (menos de 2 d√≠as de diferencia)")
        else:
            print(f"  ‚ö†Ô∏è  Los datos tienen m√°s de {(hoy - ultima_fecha).days} d√≠as")
            
    except Exception as e:
        print(f"  ‚ö†Ô∏è  Error analizando fechas: {e}")

# ======================================================
# 13. VERIFICACI√ìN DE PRON√ìSTICO COMPLETO
# ======================================================
print(f"\n{'='*80}")
print(" ‚úÖ VERIFICACI√ìN DE PRON√ìSTICO COMPLETO (72 HORAS)")
print(f"{'='*80}")

for var in variables:
    if var in archivos_guardados and archivos_guardados[var]:
        try:
            with open(archivos_guardados[var], 'r', encoding='utf-8') as f:
                datos = json.load(f)
                horas_pronostico = len(datos.get('pronostico', []))
                
                if horas_pronostico == 72:
                    print(f"‚úÖ {var}: {horas_pronostico} horas de pron√≥stico (CORRECTO)")
                else:
                    print(f"‚ö†Ô∏è  {var}: Solo {horas_pronostico} horas de pron√≥stico (DEBER√çAN SER 72)")
                    
        except Exception as e:
            print(f"‚ùå Error leyendo JSON de {var}: {e}")

print(f"\n{'='*80}")
print(" üéØ CONFIGURACI√ìN SIMPLIFICADA COMPLETADA")
print("    ‚Ä¢ Gr√°ficas con hist√≥rico y pron√≥stico 72h")
print("    ‚Ä¢ JSON con 72 horas completas de pron√≥stico")
print("    ‚Ä¢ √öltimos 1200 datos procesados")
print("    ‚Ä¢ √öltimos 2 d√≠as eliminados")
print(f"{'='*80}")
