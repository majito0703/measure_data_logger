# -*- coding: utf-8 -*-
"""Modelo_Sarima.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iAFMXztN6AvkjVaC3_b2wtbi-jGTxS8R
"""

import os
import pickle
import json
import warnings
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from datetime import datetime, timedelta

warnings.filterwarnings("ignore")

# ======================================================
# URL DE GOOGLE SHEETS (definida a nivel global)
# ======================================================
SHEET_URL = "https://docs.google.com/spreadsheets/d/1x1FeUolFWlR07tgrc6F4cgeUhJYV7uQ5yuRTBHO8jWI/edit?gid=0#gid=0"

# ======================================================
# CONFIGURACI√ìN DE CARPETAS DE SALIDA
# ======================================================
def configurar_carpetas_salida():
    """Configura las carpetas donde se guardar√°n los resultados"""
    try:
        # En Colab, guardar en Drive
        if 'IN_COLAB' in globals() and IN_COLAB:
            carpeta_base = "/content/drive/MyDrive/pronosticos"
        else:
            # En GitHub Actions o local
            carpeta_base = "./pronosticos"
        
        # Crear subcarpetas
        carpetas = {
            'base': carpeta_base,
            'graficos': os.path.join(carpeta_base, 'graficos'),
            'datos': os.path.join(carpeta_base, 'datos'),
            'modelos': os.path.join(carpeta_base, 'modelos'),
            'resumen': os.path.join(carpeta_base, 'resumen')
        }
        
        for carpeta in carpetas.values():
            os.makedirs(carpeta, exist_ok=True)
            print(f"‚úÖ Carpeta creada/verificada: {carpeta}")
        
        return carpetas
    except Exception as e:
        print(f"‚ö†Ô∏è  Error configurando carpetas: {e}")
        return None

# ======================================================
# 1. FUNCI√ìN DE CONEXI√ìN A GOOGLE SHEETS (COLAB + GITHUB)
# ======================================================


def conectar_a_google_sheets():
    """
    Conecta a Google Sheets de manera inteligente
    - En Colab: usa autenticaci√≥n normal
    - En GitHub: puede usar Service Account
    """

    try:
        # Verificar si estamos en Google Colab
        try:
            from google.colab import auth

            IN_COLAB = True
        except:
            IN_COLAB = False

        if IN_COLAB:
            # ========== MODO COLAB ==========
            print("üîë Autenticando en Google Colab...")

            # Autenticaci√≥n interactiva
            auth.authenticate_user()
            from google.auth import default

            creds, _ = default()

            # Guardar credenciales (opcional)
            with open("/content/token.pickle", "wb") as token:
                pickle.dump(creds, token)

            import gspread

            gc = gspread.authorize(creds)

        else:
            # ========== MODO GITHUB/LOCAL ==========
            import gspread

            # Intentar con variable de entorno (para GitHub Actions)
            creds_json = os.getenv("GOOGLE_SHEETS_CREDS")

            if creds_json:
                print("üîë Usando Service Account desde variable de entorno...")
                from google.oauth2.service_account import Credentials

                creds_dict = json.loads(creds_json)
                scope = ["https://www.googleapis.com/auth/spreadsheets"]
                credentials = Credentials.from_service_account_info(
                    creds_dict, scopes=scope
                )
                gc = gspread.authorize(credentials)
            else:
                # Intentar autenticaci√≥n normal (fallback)
                print("‚ö†Ô∏è  Intentando autenticaci√≥n normal...")
                gc = gspread.oauth()  # Esto abrir√° navegador en local

        # Abrir la hoja
        spreadsheet = gc.open_by_url(SHEET_URL)
        worksheet = spreadsheet.get_worksheet(0)

        print("‚úÖ Conexi√≥n exitosa a Google Sheets")
        return worksheet

    except Exception as e:
        print(f"‚ùå Error conectando a Google Sheets: {e}")
        print("‚ö†Ô∏è  Usando datos de ejemplo para continuar...")
        return None


# ======================================================
# 2. CONFIGURACI√ìN INICIAL DE COLAB
# ======================================================

# Montar Google Drive (solo funciona en Colab)
try:
    from google.colab import drive

    drive.mount("/content/drive", force_remount=False)
    IN_COLAB = True
    print("‚úÖ Google Drive montado")

except:
    IN_COLAB = False
    print("‚ö†Ô∏è  No es Google Colab, omitiendo montaje de Drive")

# ======================================================
# 3. INSTALAR DEPENDENCIAS (SOLO COLAB)
# ======================================================

if IN_COLAB:
    print("üì¶ Instalando dependencias en Colab...")
else:
    print("‚úÖ En GitHub, las dependencias se instalan desde requirements.txt")

# ======================================================
# 4. IMPORTAR BIBLIOTECAS RESTANTES
# ======================================================

import gspread
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
from statsmodels.tsa.statespace.sarimax import SARIMAX
import matplotlib.dates as mdates

# Cargar jupyter-black (solo Colab)
if IN_COLAB:
    import jupyter_black

    jupyter_black.load()

# ======================================================
# 5. REEMPLAZAR LA IMPORTACI√ìN DE 'direl_ts_tool_kit'
# ======================================================


def parse_datetime_index(df, date_column="date", format="%d/%m/%Y %H:%M:%S"):
    """
    Convierte una columna de fecha a datetime y la usa como √≠ndice
    (Reemplaza a la funci√≥n de direl_ts_tool_kit)
    """
    df_copy = df.copy()
    df_copy[date_column] = pd.to_datetime(
        df_copy[date_column], format=format, errors="coerce"
    )
    df_copy.set_index(date_column, inplace=True)
    return df_copy


def plot_time_series(df, variable, units="", time_unit="Day"):
    """
    Grafica una serie de tiempo
    (Versi√≥n simplificada de la funci√≥n original)
    """
    fig, ax = plt.subplots(figsize=(12, 5))

    ax.plot(df.index, df[variable], linewidth=1)
    ax.set_title(f"Serie de Tiempo - {variable}")
    ax.set_xlabel(f"Tiempo ({time_unit})")
    ax.set_ylabel(f"{variable} {units}")
    ax.grid(True, alpha=0.3)

    # Formato de fechas
    ax.xaxis.set_major_formatter(mdates.DateFormatter("%d/%m %H:%M"))
    plt.setp(ax.xaxis.get_majorticklabels(), rotation=45)

    plt.tight_layout()
    return fig


# ======================================================
# 6. CONECTAR Y CARGAR DATOS
# ======================================================

print("\n" + "=" * 60)
print("üì• CARGANDO DATOS DESDE GOOGLE SHEETS")
print("=" * 60)

# Conectar a Google Sheets
worksheet = conectar_a_google_sheets()

if worksheet is not None:
    # Obtener todos los datos
    datos = worksheet.get_all_values()

    # Convertir a DataFrame
    df0 = pd.DataFrame(datos[1:], columns=datos[0])

    print(f"‚úÖ Datos cargados desde Google Sheets")
    print(f"üìä Dimensiones: {df0.shape[0]} filas √ó {df0.shape[1]} columnas")
    print(f"üîó URL: {SHEET_URL}")

else:
    # ========== DATOS DE EJEMPLO (si falla la conexi√≥n) ==========
    print("üìù Usando datos de ejemplo para demostraci√≥n...")

    # Crear datos de ejemplo simples
    import numpy as np

    fechas = pd.date_range(start="2024-01-01", periods=100, freq="H")

    datos_ejemplo = {
        "Date": [f.strftime("%d/%m/%Y %H:%M:%S") for f in fechas],
        "Temperature": 25 + 5 * np.sin(np.linspace(0, 10, 100)),
        "Humidity": 60 + 10 * np.cos(np.linspace(0, 8, 100)),
        "PM 2.5(¬µg/m¬≥)": 20 + 8 * np.random.randn(100),
        "PM 10 (¬µg/m¬≥)": 40 + 12 * np.random.randn(100),
        "PM 1.0 (¬µg/m¬≥)": 10 + 4 * np.random.randn(100),
    }

    df0 = pd.DataFrame(datos_ejemplo)
    print(f"üìä Datos de ejemplo creados: {df0.shape[0]} filas")

print("\nüìã Primeras filas de datos:")
print(df0.head(2))

df0.rename(
    columns={
        "Date": "date",
        "PM 1.0 (¬µg/m¬≥)": "PM 1",
        "PM 2.5(¬µg/m¬≥)": "PM 2.5",
        "PM 10 (¬µg/m¬≥)": "PM 10",
    },
    inplace=True,
)

# ======================================================
# CONVERSI√ìN DE DATOS Y LIMPIEZA
# ======================================================

print("üîç Verificando tipos de datos iniciales:")
print(df0.dtypes)

# Identificar columnas que deber√≠an ser num√©ricas
columnas_numericas = ["Temperature", "Humidity", "PM 2.5", "PM 10", "PM 1"]

# Convertir cada columna a num√©rico, forzando errores a NaN
for col in columnas_numericas:
    if col in df0.columns:
        print(f"\nConvirtiendo columna '{col}'...")
        
        # Guardar valores originales para comparar
        valores_originales = df0[col].head(5).tolist()
        
        # Convertir a num√©rico
        df0[col] = pd.to_numeric(df0[col], errors='coerce')
        
        # Contar valores convertidos y no convertidos
        total_valores = len(df0[col])
        valores_nan = df0[col].isna().sum()
        valores_convertidos = total_valores - valores_nan
        
        print(f"  Valores originales (primeros 5): {valores_originales}")
        print(f"  Valores convertidos: {valores_convertidos}/{total_valores}")
        print(f"  Valores no convertidos (NaN): {valores_nan}")

# Crear DataFrame con √≠ndice temporal
df1 = parse_datetime_index(df0, format="%d/%m/%Y %H:%M:%S")

print("\n‚úÖ Tipos de datos despu√©s de la conversi√≥n:")
print(df1.dtypes)

# ======================================================
# LIMPIEZA ADICIONAL ANTES DEL RESAMPLE (SOLUCI√ìN AL ERROR)
# ======================================================

print("\nüßπ Realizando limpieza adicional antes del resample...")

# 1. Eliminar columnas vac√≠as o con nombres vac√≠os
df1 = df1.loc[:, ~df1.columns.astype(str).str.contains('^Unnamed')]
df1 = df1.loc[:, ~df1.columns.astype(str).str.contains('^$')]

# 2. Seleccionar solo columnas num√©ricas conocidas
columnas_a_mantener = ['Temperature', 'Humidity', 'PM 1', 'PM 2.5', 'PM 10']
columnas_existentes = [col for col in columnas_a_mantener if col in df1.columns]

print(f"üìã Columnas a mantener: {columnas_existentes}")

# 3. Verificar que todas las columnas sean num√©ricas
for col in columnas_existentes:
    if df1[col].dtype not in ['int64', 'float64']:
        print(f"‚ö†Ô∏è  Columna '{col}' no es num√©rica, convirtiendo...")
        df1[col] = pd.to_numeric(df1[col], errors='coerce')

# 4. Eliminar filas donde todas las columnas sean NaN
df1 = df1.dropna(how='all')

print(f"\nüìä DataFrame despu√©s de limpieza:")
print(f"  - Forma: {df1.shape[0]} filas √ó {df1.shape[1]} columnas")
print(f"  - Tipos de datos: {df1.dtypes.to_dict()}")

# ======================================================
# RESAMPLE CON SEGURIDAD
# ======================================================

print("\nüìä Realizando resample a 10 minutos...")
try:
    # Opci√≥n 1: Usar numeric_only=True para seguridad
    df1 = df1.resample("10min").mean(numeric_only=True)
    print("‚úÖ Resample completado exitosamente con numeric_only=True")
    
except Exception as e:
    print(f"‚ö†Ô∏è  Error con numeric_only=True: {e}")
    print("üîÑ Intentando m√©todo alternativo...")
    
    # Opci√≥n 2: M√©todo alternativo manual
    try:
        # Crear lista para almacenar resultados
        resample_data = []
        
        # Para cada columna num√©rica
        for col in df1.columns:
            if df1[col].dtype in ['int64', 'float64']:
                # Resample por columna
                col_resampled = df1[col].resample("10min").mean()
                resample_data.append(col_resampled)
        
        # Combinar resultados
        df1 = pd.concat(resample_data, axis=1)
        print("‚úÖ Resample completado exitosamente con m√©todo alternativo")
        
    except Exception as e2:
        print(f"‚ùå Error en m√©todo alternativo: {e2}")
        print("‚ö†Ô∏è  Continuando sin resample...")
        # Mantener df1 sin cambios

print(f"üìä Nueva forma despu√©s del resample: {df1.shape[0]} filas √ó {df1.shape[1]} columnas")

# Mostrar estad√≠sticas
print("\nüìà Estad√≠sticas descriptivas:")
print(df1.describe().transpose())

fig = plot_time_series(df1, variable="Temperature", units="¬∞C", time_unit="Day")
plt.show()

fig = plot_time_series(df1, variable="Humidity", units="%", time_unit="Day")
plt.show()

fig = plot_time_series(df1, variable="PM 10", units="¬µg/m¬≥", time_unit="Day")
plt.show()

fig = plot_time_series(df1, variable="PM 2.5", units="¬µg/m¬≥", time_unit="Day")
plt.show()

df2 = df1.copy()

vars_to_impute = ["Temperature", "Humidity", "PM 10", "PM 2.5"]

for var in vars_to_impute:
    # Marcar NaN antes de imputar
    df2[f"{var}_imputed"] = df2[var].isna()

    # Calcular medias por hora:minuto
    means = df2.groupby(df2.index.time)[var].transform("mean")

    # Llenar SOLO los NaN, sin alterar valores originales
    df2[var] = df2[var].fillna(means)

df2 = df2.loc[:, ~df2.columns.str.endswith("_imputed")]
# Ver primeras filas del nuevo DataFrame
print(df2.head())

# Ver solo los datos imputados de cada variable
print(df2[[col for col in df2.columns if "imputed" in col]].sum())
df2 = df2.loc[:, ~df2.columns.str.endswith("_imputed")]

fig = plot_time_series(df2, variable="Temperature", units="¬∞C", time_unit="Day")
plt.show()

fig = plot_time_series(df2, variable="Humidity", units="%", time_unit="Day")
plt.show()

fig = plot_time_series(df2, variable="PM 10", units="¬µg/m¬≥", time_unit="Day")
plt.show()

fig = plot_time_series(df2, variable="PM 2.5", units="¬µg/m¬≥", time_unit="Day")
plt.show()

df3 = df2.copy()
# Eliminar los d√≠as 24 y 25
df3 = df3[~df3.index.day.isin([24, 25])]
df3 = df3.drop(columns=["PM 1"])
# Verificar los primeros registros
print(df3.head())

# Verificar los √∫ltimos registros para asegurarte que se eliminaron los d√≠as 19 y 20
print(df3.tail())

# ======================================================
# CONFIGURAR CARPETAS DE SALIDA
# ======================================================
print("\n" + "=" * 60)
print("üìÅ CONFIGURANDO CARPETAS DE SALIDA")
print("=" * 60)

carpetas = configurar_carpetas_salida()

# ======================================================
# 1. RESAMPLEO A DATOS POR HORA CON FRECUENCIA EXPL√çCITA
# ======================================================
# Asegurar que el √≠ndice tenga frecuencia para SARIMA
df_hourly = df3.resample("H").mean().dropna()

# Asignar frecuencia expl√≠cita al √≠ndice (IMPORTANTE PARA SARIMA)
if not df_hourly.index.freq:
    print("‚ö†Ô∏è  Asignando frecuencia horaria al √≠ndice...")
    df_hourly = df_hourly.asfreq('H')

variables = ["Temperature", "Humidity", "PM 2.5", "PM 10"]

# ======================================================
# 2. OPTIMIZADOR SARIMA (SIN PMDARIMA)
# ======================================================
p = d = q = [0, 1]
P = D = Q = [0, 1]
m = 12  # estacionalidad de 12 horas


def buscar_mejor_modelo(series):
    mejor_aic = float("inf")
    mejor_modelo = None
    mejor_orden = None
    mejor_orden_season = None
    mejores_parametros = None
    mejor_summary = None

    for pi in p:
        for di in d:
            for qi in q:
                for Pi in P:
                    for Di in D:
                        for Qi in Q:
                            orden = (pi, di, qi)
                            orden_seas = (Pi, Di, Qi, m)

                            try:
                                modelo = SARIMAX(
                                    series,
                                    order=orden,
                                    seasonal_order=orden_seas,
                                    enforce_stationarity=False,
                                    enforce_invertibility=False,
                                ).fit(disp=False, maxiter=200)

                                if modelo.aic < mejor_aic:
                                    mejor_aic = modelo.aic
                                    mejor_modelo = modelo
                                    mejor_orden = orden
                                    mejor_orden_season = orden_seas
                                    mejores_parametros = modelo.params
                                    mejor_summary = modelo.summary()

                            except:
                                continue

    return (
        mejor_modelo,
        mejor_orden,
        mejor_orden_season,
        mejor_aic,
        mejores_parametros,
        mejor_summary,
    )

# ======================================================
# 3. FUNCI√ìN PARA OBTENER ECUACI√ìN MATEM√ÅTICA
# ======================================================
def obtener_ecuacion_sarima(modelo, orden, orden_seas):
    """
    Genera la ecuaci√≥n matem√°tica del modelo SARIMA
    """
    p, d, q = orden
    P, D, Q, m = orden_seas

    # Obtener par√°metros
    params = modelo.params

    # Inicializar partes de la ecuaci√≥n
    parte_ar = ""
    parte_ma = ""
    parte_sar = ""
    parte_sma = ""

    # Coeficientes AR (no estacional)
    for i in range(1, p + 1):
        if f"ar.L{i}" in params:
            coef = params[f"ar.L{i}"]
            parte_ar += f" + {coef:.4f}¬∑y_t-{i}"

    # Coeficientes MA (no estacional)
    for i in range(1, q + 1):
        if f"ma.L{i}" in params:
            coef = params[f"ma.L{i}"]
            parte_ma += f" + {coef:.4f}¬∑Œµ_t-{i}"

    # Coeficientes SAR (estacional)
    for i in range(1, P + 1):
        if f"ar.S.L{m*i}" in params:
            coef = params[f"ar.S.L{m*i}"]
            parte_sar += f" + {coef:.4f}¬∑y_t-{m*i}"

    # Coeficientes SMA (estacional)
    for i in range(1, Q + 1):
        if f"ma.S.L{m*i}" in params:
            coef = params[f"ma.S.L{m*i}"]
            parte_sma += f" + {coef:.4f}¬∑Œµ_t-{m*i}"

    # Constante
    constante = ""
    if "intercept" in params:
        constante = f"{params['intercept']:.4f} + "
    elif "const" in params:
        constante = f"{params['const']:.4f} + "

    # Construir ecuaci√≥n
    if d == 0 and D == 0:
        ecuacion = f"y_t = {constante}"
    else:
        # Para modelos con diferenciaci√≥n
        ecuacion = "Œî^d Œî_s^D y_t = "
        if constante.strip():
            ecuacion = f"Œî^d Œî_s^D y_t = {constante}"

    # Agregar partes
    if parte_ar:
        ecuacion += parte_ar[3:] if ecuacion.endswith("= ") else parte_ar
    if parte_ma:
        ecuacion += parte_ma
    if parte_sar:
        ecuacion += parte_sar
    if parte_sma:
        ecuacion += parte_sma

    if parte_ar or parte_ma or parte_sar or parte_sma:
        ecuacion += " + Œµ_t"
    else:
        ecuacion += "Œµ_t"

    return ecuacion

# ======================================================
# 4. FUNCI√ìN PARA MOSTRAR PAR√ÅMETROS COMO EN LA IMAGEN
# ======================================================
def mostrar_parametros_tabla(modelo, orden, orden_seas, aic):
    """
    Muestra los par√°metros en formato de tabla como en la imagen
    """
    params = modelo.params
    p, d, q = orden
    P, D, Q, m = orden_seas

    print("\n" + "=" * 60)
    print("PAR√ÅMETROS DEL MODELO SARIMA")
    print("=" * 60)

    # Crear lista de par√°metros
    parametros_lista = []

    # Par√°metros AR
    for i in range(1, p + 1):
        key = f"ar.L{i}"
        if key in params:
            parametros_lista.append((f"œÜ_{i}", params[key], modelo.bse.get(key, "N/A")))

    # Par√°metros MA
    for i in range(1, q + 1):
        key = f"ma.L{i}"
        if key in params:
            parametros_lista.append((f"Œ∏_{i}", params[key], modelo.bse.get(key, "N/A")))

    # Par√°metros SAR
    for i in range(1, P + 1):
        key = f"ar.S.L{m*i}"
        if key in params:
            parametros_lista.append((f"Œ¶_{i}", params[key], modelo.bse.get(key, "N/A")))

    # Par√°metros SMA
    for i in range(1, Q + 1):
        key = f"ma.S.L{m*i}"
        if key in params:
            parametros_lista.append((f"Œò_{i}", params[key], modelo.bse.get(key, "N/A")))

    # Constante
    if "intercept" in params:
        parametros_lista.append(
            ("intercept", params["intercept"], modelo.bse.get("intercept", "N/A"))
        )
    elif "const" in params:
        parametros_lista.append(
            ("const", params["const"], modelo.bse.get("const", "N/A"))
        )

    # Varianza del error
    if "sigma2" in params:
        parametros_lista.append(
            ("œÉ¬≤", params["sigma2"], modelo.bse.get("sigma2", "N/A"))
        )

    # Mostrar tabla
    print(f"\nOrden: SARIMA{orden}{orden_seas}")
    print(f"AIC: {aic:.2f}")
    print("\n" + "-" * 60)
    print(f"{'Par√°metro':<15} {'Valor':<15} {'Error est√°ndar':<15}")
    print("-" * 60)

    for nombre, valor, error in parametros_lista:
        if isinstance(error, (int, float)):
            print(f"{nombre:<15} {valor:<15.4f} {error:<15.4f}")
        else:
            print(f"{nombre:<15} {valor:<15.4f} {str(error):<15}")

    print("-" * 60)

# ======================================================
# 5. FUNCIONES PARA GUARDAR RESULTADOS
# ======================================================
def guardar_pronostico_csv(modelo, serie, variable, pasos=72):
    """Guarda el pron√≥stico en un archivo CSV"""
    try:
        if carpetas is None:
            print("‚ö†Ô∏è  No se pudieron crear carpetas, no se guardar√° el pron√≥stico")
            return None
            
        pred = modelo.get_forecast(steps=pasos)
        media = pred.predicted_mean
        conf_95 = pred.conf_int(alpha=0.05)
        
        # Crear DataFrame con el pron√≥stico
        df_pronostico = pd.DataFrame({
            'fecha': media.index,
            'pronostico': media.values,
            'limite_inferior_95': conf_95.iloc[:, 0],
            'limite_superior_95': conf_95.iloc[:, 1]
        })
        
        # Formatear fechas
        df_pronostico['fecha'] = pd.to_datetime(df_pronostico['fecha'])
        df_pronostico['fecha_str'] = df_pronostico['fecha'].dt.strftime('%Y-%m-%d %H:%M:%S')
        
        # Agregar informaci√≥n adicional
        df_pronostico['variable'] = variable
        df_pronostico['fecha_generacion'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        df_pronostico['horizonte_pronostico'] = f'{pasos} horas'
        
        # Reordenar columnas
        df_pronostico = df_pronostico[[
            'variable', 'fecha', 'fecha_str', 'pronostico',
            'limite_inferior_95', 'limite_superior_95',
            'fecha_generacion', 'horizonte_pronostico'
        ]]
        
        # Guardar en CSV
        fecha_actual = datetime.now().strftime('%Y%m%d_%H%M')
        nombre_archivo = f"pronostico_{variable}_{fecha_actual}.csv"
        ruta_completa = os.path.join(carpetas['datos'], nombre_archivo)
        
        df_pronostico.to_csv(ruta_completa, index=False, encoding='utf-8')
        print(f"‚úÖ Pron√≥stico guardado en: {ruta_completa}")
        
        return ruta_completa
        
    except Exception as e:
        print(f"‚ö†Ô∏è  Error guardando pron√≥stico para {variable}: {e}")
        return None

def guardar_grafico_pronostico(fig, variable):
    """Guarda el gr√°fico del pron√≥stico"""
    try:
        if carpetas is None:
            return None
            
        fecha_actual = datetime.now().strftime('%Y%m%d_%H%M')
        nombre_archivo = f"pronostico_{variable}_{fecha_actual}.png"
        ruta_completa = os.path.join(carpetas['graficos'], nombre_archivo)
        
        fig.savefig(ruta_completa, dpi=150, bbox_inches='tight')
        print(f"‚úÖ Gr√°fico guardado en: {ruta_completa}")
        
        return ruta_completa
        
    except Exception as e:
        print(f"‚ö†Ô∏è  Error guardando gr√°fico para {variable}: {e}")
        return None

def guardar_modelo(modelo, variable, orden, orden_seas, aic):
    """Guarda el modelo en formato pickle"""
    try:
        if carpetas is None:
            return None
            
        fecha_actual = datetime.now().strftime('%Y%m%d_%H%M')
        nombre_archivo = f"modelo_{variable}_{fecha_actual}.pkl"
        ruta_completa = os.path.join(carpetas['modelos'], nombre_archivo)
        
        # Guardar informaci√≥n del modelo
        info_modelo = {
            'modelo': modelo,
            'variable': variable,
            'orden': orden,
            'orden_estacional': orden_seas,
            'aic': aic,
            'fecha_entrenamiento': datetime.now(),
            'parametros': modelo.params.to_dict(),
            'resumen': str(modelo.summary())
        }
        
        with open(ruta_completa, 'wb') as f:
            pickle.dump(info_modelo, f)
        
        print(f"‚úÖ Modelo guardado en: {ruta_completa}")
        
        return ruta_completa
        
    except Exception as e:
        print(f"‚ö†Ô∏è  Error guardando modelo para {variable}: {e}")
        return None

def crear_resumen_general(resultados, ecuaciones, df_hourly, variables):
    """Crea un resumen general de todos los modelos"""
    try:
        if carpetas is None:
            return None
            
        fecha_actual = datetime.now().strftime('%Y%m%d_%H%M')
        
        # Crear resumen en formato CSV
        resumen_data = []
        
        for var in variables:
            if var in resultados:
                modelo = resultados[var]
                resumen_data.append({
                    'variable': var,
                    'orden': str(modelo.specification.order),
                    'orden_estacional': str(modelo.specification.seasonal_order),
                    'aic': modelo.aic,
                    'num_observaciones': len(df_hourly[var]),
                    'ecuacion': ecuaciones.get(var, 'No disponible'),
                    'fecha_ultimo_dato': df_hourly.index[-1].strftime('%Y-%m-%d %H:%M:%S'),
                    'valor_ultimo_dato': df_hourly[var].iloc[-1]
                })
        
        df_resumen = pd.DataFrame(resumen_data)
        
        # Guardar en CSV
        ruta_csv = os.path.join(carpetas['resumen'], f"resumen_modelos_{fecha_actual}.csv")
        df_resumen.to_csv(ruta_csv, index=False, encoding='utf-8')
        
        # Guardar en JSON tambi√©n
        ruta_json = os.path.join(carpetas['resumen'], f"resumen_modelos_{fecha_actual}.json")
        df_resumen.to_json(ruta_json, orient='records', indent=2)
        
        print(f"‚úÖ Resumen guardado en: {ruta_csv}")
        print(f"‚úÖ Resumen JSON guardado en: {ruta_json}")
        
        return ruta_csv
        
    except Exception as e:
        print(f"‚ö†Ô∏è  Error creando resumen general: {e}")
        return None

# ======================================================
# 6. FUNCI√ìN PARA GRAFICAR PRON√ìSTICO CON MANEJO DE FECHAS
# ======================================================
def graficar_pronostico(modelo, serie, variable, pasos=48, limite=None):
    try:
        pred = modelo.get_forecast(steps=pasos)
        media = pred.predicted_mean
        conf_80 = pred.conf_int(alpha=0.20)
        conf_95 = pred.conf_int(alpha=0.05)

        # Crear figura con tama√±o adecuado
        fig, ax = plt.subplots(figsize=(15, 6))

        # Datos hist√≥ricos
        ax.plot(serie.index, serie.values, label="Medido", color="black", linewidth=1.5)

        # Verificar si el pron√≥stico tiene √≠ndice de fechas
        if hasattr(media.index, 'freq') and media.index.freq is not None:
            # El pron√≥stico tiene fechas
            fecha_min = serie.index.min()
            fecha_max = media.index.max()
            
            # Pron√≥stico
            ax.plot(
                media.index,
                media.values,
                label=f"Pron√≥stico {variable}",
                color="red",
                linewidth=2,
            )

            # Bandas de confianza
            ax.fill_between(
                conf_80.index,
                conf_80.iloc[:, 0],
                conf_80.iloc[:, 1],
                color="green",
                alpha=0.3,
                label="Confianza 80%",
            )
            ax.fill_between(
                conf_95.index,
                conf_95.iloc[:, 0],
                conf_95.iloc[:, 1],
                color="yellow",
                alpha=0.2,
                label="Confianza 95%",
            )

            # L√≠nea vertical para separar historial de pron√≥stico
            ultimo_historial = serie.index[-1]
            ax.axvline(x=ultimo_historial, color="gray", linestyle="--", alpha=0.7, linewidth=1)

            # Configurar formato de fechas
            # Calcular diferencia de d√≠as para determinar el formato
            dias_totales = (fecha_max - fecha_min).days

            if dias_totales <= 7:  # Si es menos de una semana
                # Formato: D√≠a Hora (ej: "11 Nov 10:00")
                ax.xaxis.set_major_formatter(mdates.DateFormatter("%d %b %H:%M"))
                ax.xaxis.set_major_locator(mdates.HourLocator(interval=6))
            elif dias_totales <= 30:  # Si es menos de un mes
                # Formato: D√≠a Mes (ej: "11 Nov")
                ax.xaxis.set_major_formatter(mdates.DateFormatter("%d %b"))
                ax.xaxis.set_major_locator(mdates.DayLocator(interval=2))
            else:  # Si es m√°s de un mes
                # Formato: Mes (ej: "Nov 2025")
                ax.xaxis.set_major_formatter(mdates.DateFormatter("%b %Y"))
                ax.xaxis.set_major_locator(mdates.MonthLocator())

            # Agregar leyenda de separaci√≥n
            ax.text(
                ultimo_historial + timedelta(hours=1),
                ax.get_ylim()[1] * 0.95,
                "Pron√≥stico",
                fontsize=10,
                color="darkred",
                alpha=0.8,
            )
        else:
            # El pron√≥stico no tiene fechas, usar √≠ndice num√©rico
            print(f"‚ö†Ô∏è  Pron√≥stico para {variable} no tiene fechas, usando √≠ndice num√©rico")
            
            # Crear fechas para el pron√≥stico
